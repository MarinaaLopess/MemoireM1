{"cells":[{"cell_type":"code","execution_count":null,"id":"ae90f3e2-42f2-42a8-8b53-d0f755580e7f","metadata":{"id":"ae90f3e2-42f2-42a8-8b53-d0f755580e7f","outputId":"0f696a6d-8ce3-4544-885b-1cc9c2c1dae3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n","Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n","Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.1)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"id":"1099ca0f-4296-4716-8472-bf7f7d0e1386","metadata":{"id":"1099ca0f-4296-4716-8472-bf7f7d0e1386","outputId":"6fd50278-3eea-4c53-bc9c-db81eaa87268"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('Barack', 'PERSON'), ('Obama', 'PERSON'), ('was', 'O'), ('born', 'O'), ('in', 'O'), ('Hawaii', 'LOCATION'), ('.', 'O'), ('He', 'O'), ('was', 'O'), ('the', 'O'), ('44th', 'O'), ('President', 'O'), ('of', 'O'), ('the', 'O'), ('United', 'LOCATION'), ('States', 'LOCATION'), ('.', 'O')]\n"]}],"source":["import nltk\n","from nltk.tag import StanfordNERTagger\n","\n","# Chemin du modèle Stanford NER\n","#stanford_classifier = \"stanford-ner-4.2.0/classifiers/english.all.3class.distsim.crf.ser.gz\"\n","stanford_classifier = \"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz\"\n","#stanford_classifier = \"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/classifiers/french-ner.ser.gz\"  # Modèle français\n","stanford_ner_path =\"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/stanford-ner.jar\"\n","\n","# Créer le tagger NER\n","ner_tagger = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n","\n","# Texte d'exemple\n","text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n","\n","# Tokenisation\n","words = nltk.word_tokenize(text)\n","\n","# Effectuer l'étiquetage NER\n","ner_results = ner_tagger.tag(words)\n","\n","# Afficher les résultats\n","print(ner_results)\n"]},{"cell_type":"markdown","id":"83950134-fd18-41c3-909d-bd79966a5f6a","metadata":{"id":"83950134-fd18-41c3-909d-bd79966a5f6a"},"source":["Le modèle Stanford NER génère des résultats d'étiquetage des entités nommées (NER) pour le texte. Chaque tuple contient deux éléments : un mot (ou un symbole) et le type d'entité nommée associé à ce mot.\n","\n","PERSON indique un nom de personne (comme 'Barack' et 'Obama').\n","LOCATION indique un nom de lieu (comme 'Hawaii' et 'United States').\n","O indique une catégorie \"autre\", ce qui signifie que ce mot n'est pas une entité nommée ou qu'il ne correspond à aucune catégorie spécifique.\n"]},{"cell_type":"code","execution_count":null,"id":"a1c8ea2e-d199-462c-8c35-988d0a4d151d","metadata":{"id":"a1c8ea2e-d199-462c-8c35-988d0a4d151d","outputId":"66a93555-e39a-4142-af11-a231c0203fa1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Entités   : 329\n","Candidats : 15393\n","[{'mot': 'Texte', 'type': 'ORGANIZATION'}, {'mot': 'Vues', 'type': 'ORGANIZATION'}, {'mot': 'Calligrammes', 'type': 'ORGANIZATION'}, {'mot': '', 'type': 'ORGANIZATION'}, {'mot': 'Guillaume', 'type': 'ORGANIZATION'}, {'mot': 'Apollinaire', 'type': 'ORGANIZATION'}, {'mot': 'Guillaume', 'type': 'PERSON'}, {'mot': 'Lausanne', 'type': 'LOCATION'}, {'mot': 'Roger', 'type': 'PERSON'}, {'mot': 'France', 'type': 'LOCATION'}, {'mot': 'Il', 'type': 'PERSON'}, {'mot': 'GUILLAUME', 'type': 'PERSON'}, {'mot': 'APOLLINAIRE', 'type': 'PERSON'}, {'mot': 'Cordes', 'type': 'PERSON'}, {'mot': 'Cordes', 'type': 'PERSON'}, {'mot': 'Cordes', 'type': 'PERSON'}, {'mot': 'Cordes', 'type': 'PERSON'}, {'mot': 'Bigorneaux', 'type': 'LOCATION'}, {'mot': 'Lotte', 'type': 'LOCATION'}, {'mot': 'Puits', 'type': 'PERSON'}]\n","--------------------------------------------------\n","['Rappel', 'de', 'votre', 'demande', ':', 'Format', 'de', 'téléchargement', ':', ':', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages', ':', '262']\n"]}],"source":["import nltk\n","from nltk.tag import StanfordNERTagger\n","import re\n","\n","\n","stanford_classifier = \"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz\"\n","stanford_ner_path = \"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/stanford-ner.jar\"\n","\n","\n","ner_tagger = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n","\n","with open(\"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\", \"r\", encoding=\"utf-8\") as f:\n","    texte = f.read().strip()\n","\n","liste_entitesnom = []\n","liste_candidat = []\n","\n","words = nltk.word_tokenize(texte)\n","ner_results = ner_tagger.tag(words)\n","for word, tag in ner_results:\n","    if tag != \"O\":\n","        liste_entitesnom.append({\"mot\": word, \"type\": tag})\n","    else:\n","        liste_candidat.append(word)\n","\n","print(f\"Entités   : {len(liste_entitesnom)}\")\n","print(f\"Candidats : {len(liste_candidat )}\")\n","print(liste_entitesnom[:20])\n","print(\"-\"*50)\n","print(list(liste_candidat )[:20])\n"]},{"cell_type":"markdown","id":"a77922e2-7d0f-4b87-8a03-0dbf63b4d1f9","metadata":{"id":"a77922e2-7d0f-4b87-8a03-0dbf63b4d1f9"},"source":["Étant donné que Stanford NER n'a pas de modèle de traitement officiel pour le français, le modèle utilisé actuellement est celui en anglais. Cependant, selon les résultats d'annotation, le modèle en anglais génère de nombreuses erreurs de marquage."]},{"cell_type":"markdown","id":"f25aefc9-e119-44a9-81ea-7ba830fceefb","metadata":{"id":"f25aefc9-e119-44a9-81ea-7ba830fceefb"},"source":["#### Stanza est un package de traitement du langage naturel développé par l'Université de Stanford, offrant un support multilingue basé sur l'apprentissage profond, y compris pour le français. Nous pouvons utiliser la bibliothèque Stanza pour effectuer la reconnaissance d'entités nommées. https://github.com/These-SCAI2023/CORPUS/blob/master/prog/Use_stanza.py\n","\n"]},{"cell_type":"code","execution_count":null,"id":"70e54630-d3ef-42d5-91e2-ef5bbf33ee7e","metadata":{"id":"70e54630-d3ef-42d5-91e2-ef5bbf33ee7e"},"outputs":[],"source":["import stanza\n","def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n","    try:\n","        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    except:\n","        stanza.download(lang=lang, logging_level='DEBUG')\n","        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    return nlp"]},{"cell_type":"code","execution_count":null,"id":"e29d915d-928c-4f76-95a8-ad4fb2fde0eb","metadata":{"scrolled":true,"colab":{"referenced_widgets":["6d0cd9840af4450c818f55b5e801376a"]},"id":"e29d915d-928c-4f76-95a8-ad4fb2fde0eb","outputId":"994a4bcb-27f4-434e-e2f0-649cdcb35679"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-09 19:54:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d0cd9840af4450c818f55b5e801376a","version_major":2,"version_minor":0},"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2025-04-09 19:54:11 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n","2025-04-09 19:54:11 WARNING: Language fr package default expects mwt, which has been added\n","2025-04-09 19:54:11 INFO: Loading these models for language: fr (French):\n","==================================\n","| Processor | Package            |\n","----------------------------------\n","| tokenize  | combined           |\n","| mwt       | combined           |\n","| ner       | wikinergold_charlm |\n","==================================\n","\n","2025-04-09 19:54:11 INFO: Using device: cpu\n","2025-04-09 19:54:11 INFO: Loading: tokenize\n","2025-04-09 19:54:11 INFO: Loading: mwt\n","2025-04-09 19:54:11 INFO: Loading: ner\n","2025-04-09 19:54:13 INFO: Done loading processors!\n"]},{"name":"stdout","output_type":"stream","text":["Entités : 584\n","[{'entite_0': {'label': 'MISC', 'text': 'Calligrammes', 'jalons': [132, 144]}}, {'entite_1': {'label': 'PER', 'text': 'Guillaume Apollinaire', 'jalons': [147, 168]}}, {'entite_2': {'label': 'PER', 'text': 'La Fresnaye', 'jalons': [182, 193]}}, {'entite_3': {'label': 'PER', 'text': 'Apollinaire', 'jalons': [204, 215]}}, {'entite_4': {'label': 'PER', 'text': 'Guillaume', 'jalons': [217, 226]}}, {'entite_5': {'label': 'LOC', 'text': 'Lausanne', 'jalons': [268, 276]}}, {'entite_7': {'label': 'PER', 'text': 'Roger de', 'jalons': [330, 338]}}, {'entite_8': {'label': 'MISC', 'text': 'Langue : Français', 'jalons': [550, 567]}}, {'entite_9': {'label': 'MISC', 'text': 'Format', 'jalons': [569, 575]}}, {'entite_11': {'label': 'MISC', 'text': 'Description : Collection : Collection du Bouquet', 'jalons': [699, 747]}}, {'entite_13': {'label': 'MISC', 'text': 'Droits : Public domain', 'jalons': [918, 940]}}, {'entite_14': {'label': 'MISC', 'text': 'Identifiant', 'jalons': [942, 953]}}, {'entite_15': {'label': 'LOC', 'text': 'Bibliothèque nationale de France', 'jalons': [991, 1023]}}, {'entite_16': {'label': 'ORG', 'text': 'Littérature et art', 'jalons': [1037, 1055]}}, {'entite_17': {'label': 'MISC', 'text': 'Conservation numérique', 'jalons': [1072, 1094]}}, {'entite_19': {'label': 'MISC', 'text': 'OCR', 'jalons': [1359, 1362]}}, {'entite_20': {'label': 'PER', 'text': 'GUILLAUME APOLLINAIRE', 'jalons': [1432, 1453]}}, {'entite_21': {'label': 'PER', 'text': 'CALLIGRAMMES', 'jalons': [1459, 1471]}}, {'entite_23': {'label': 'LOC', 'text': 'M E R M O D', 'jalons': [1497, 1508]}}, {'entite_28': {'label': 'PER', 'text': 'RENÉ DALIZE', 'jalons': [1642, 1653]}}]\n"]}],"source":["import nltk\n","import stanza\n","import re\n","import json\n","\n","# Charger le modèle Stanza\n","def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n","    try:\n","        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    except:\n","        stanza.download(lang=lang, logging_level='DEBUG')\n","        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    return nlp\n","\n","# Lire le texte à partir d'un fichier\n","def lire_fichier(chemin, is_json=False):\n","    with open(chemin, encoding='utf-8') as f:\n","        if is_json:\n","            return json.load(f)\n","        else:\n","            return f.read().strip()\n","\n","# Obtenir un dictionnaire représentant une entité nommée\n","def get_ent_dict(ent) -> dict:\n","    return {\"label\": ent.type, \"text\": ent.text, \"jalons\": [ent.start_char, ent.end_char]}\n","\n","# Traiter le texte et retourner un dictionnaire des entités\n","def dico_resultats(text, lang: str = \"fr\") -> dict:\n","    nlp = load_stanza_model(lang=lang)\n","    doc = nlp(text)\n","    # Éviter les doublons en se basant sur le texte et le type\n","    entites_unique = []\n","    seen_entities = set()  # Pour suivre les entités déjà traitées\n","\n","    for i, ent in enumerate(doc.ents):\n","        entity_key = (ent.text, ent.type)\n","        if entity_key not in seen_entities:\n","            entites_unique.append({f\"entite_{i}\": get_ent_dict(ent)})\n","            seen_entities.add(entity_key)\n","\n","    return entites_unique\n","\n","# Obtenir le format d’étiquetage BIO\n","def bio_stanza(text: str, lang: str = \"fr\") -> list[str]:\n","    nlp = load_stanza_model(lang=lang)\n","    doc = nlp(text)\n","    return [\n","        [token.text, token.ner] for sentence in doc.sentences for token in sentence.tokens\n","    ]\n","\n","if __name__ == \"__main__\":\n","    # Chemin du fichier d'entrée\n","    file_path = \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\"  # Veuillez le modifier selon votre chemin réel\n","    texte = lire_fichier(file_path)\n","\n","    # Obtenir les entités nommées\n","    entites = dico_resultats(texte, lang=\"fr\")\n","    print(f\"Entités : {len(entites)}\")\n","    print(entites[:20])  # Afficher les 20 premières entités\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["import json\n","import os\n","\n","output_json_path = \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes_entites.json\"\n","\n","# Sauvegarder les entités dans un fichier JSON\n","with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n","    json.dump(entites, json_file, ensure_ascii=False, indent=4)\n","\n","print(f\"Les entités nommées ont été sauvegardées dans : {output_json_path}\")\n"],"metadata":{"id":"EBqk8kUo9zkN"},"id":"EBqk8kUo9zkN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ece07bc4-173f-43c0-9fb6-81f04404f7ad","metadata":{"scrolled":true,"colab":{"referenced_widgets":["055501db64074e90b7b965f3c49f366c","d3719cbe5f9b4d848ae98a47b76e81af"]},"id":"ece07bc4-173f-43c0-9fb6-81f04404f7ad","outputId":"50387fc7-0545-4548-809a-2416f1ee9b1c"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-11 07:11:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"055501db64074e90b7b965f3c49f366c","version_major":2,"version_minor":0},"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2025-04-11 07:11:01 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n","2025-04-11 07:11:01 WARNING: Language fr package default expects mwt, which has been added\n","2025-04-11 07:11:01 INFO: Loading these models for language: fr (French):\n","==================================\n","| Processor | Package            |\n","----------------------------------\n","| tokenize  | combined           |\n","| mwt       | combined           |\n","| ner       | wikinergold_charlm |\n","==================================\n","\n","2025-04-11 07:11:01 INFO: Using device: cpu\n","2025-04-11 07:11:01 INFO: Loading: tokenize\n","2025-04-11 07:11:01 INFO: Loading: mwt\n","2025-04-11 07:11:01 INFO: Loading: ner\n","2025-04-11 07:11:04 INFO: Done loading processors!\n","2025-04-11 07:11:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"name":"stdout","output_type":"stream","text":["Entités : 584\n","[{'mot': 'Calligrammes', 'type': 'MISC'}, {'mot': 'Guillaume Apollinaire', 'type': 'PER'}, {'mot': 'La Fresnaye', 'type': 'PER'}, {'mot': 'Apollinaire', 'type': 'PER'}, {'mot': 'Guillaume', 'type': 'PER'}, {'mot': 'Lausanne', 'type': 'LOC'}, {'mot': 'Roger de', 'type': 'PER'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Description : Collection : Collection du Bouquet', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'GUILLAUME APOLLINAIRE', 'type': 'PER'}, {'mot': 'CALLIGRAMMES', 'type': 'PER'}, {'mot': 'M E R M O D', 'type': 'LOC'}, {'mot': 'RENÉ DALIZE', 'type': 'PER'}]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3719cbe5f9b4d848ae98a47b76e81af","version_major":2,"version_minor":0},"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2025-04-11 07:11:19 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n","2025-04-11 07:11:19 WARNING: Language fr package default expects mwt, which has been added\n","2025-04-11 07:11:19 INFO: Loading these models for language: fr (French):\n","==================================\n","| Processor | Package            |\n","----------------------------------\n","| tokenize  | combined           |\n","| mwt       | combined           |\n","| ner       | wikinergold_charlm |\n","==================================\n","\n","2025-04-11 07:11:19 INFO: Using device: cpu\n","2025-04-11 07:11:19 INFO: Loading: tokenize\n","2025-04-11 07:11:19 INFO: Loading: mwt\n","2025-04-11 07:11:19 INFO: Loading: ner\n","2025-04-11 07:11:22 INFO: Done loading processors!\n"]},{"name":"stdout","output_type":"stream","text":["\n","Tokens ：\n","16683\n","Rappel\n","de\n","votre\n","demande\n",":\n","Format\n","de\n","téléchargement\n",":\n",":\n","Texte\n","Vues\n","1\n","à\n","262\n","sur\n","262\n","Nombre\n","de\n","pages\n","\n","Candidats ：\n","16004\n","['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages', '262', 'Notice', 'complète', 'Titre']\n"]}],"source":["import nltk\n","import stanza\n","import re\n","import json\n","\n","# Charger le modèle Stanza\n","def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n","    try:\n","        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    except:\n","        stanza.download(lang=lang, logging_level='DEBUG')\n","        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    return nlp\n","\n","# Lire le texte à partir d'un fichier\n","def lire_fichier(chemin, is_json=False):\n","    with open(chemin, encoding='utf-8') as f:\n","        if is_json:\n","            return json.load(f)\n","        else:\n","            return f.read().strip()\n","\n","# Obtenir un dictionnaire représentant une entité nommée (modifié selon la structure demandée)\n","def get_ent_dict(ent) -> dict:\n","    return {\"mot\": ent.text, \"type\": ent.type}\n","\n","# Traiter le texte et retourner une liste d'entités sans doublons\n","def dico_resultats(text, lang: str = \"fr\") -> dict:\n","    nlp = load_stanza_model(lang=lang)\n","    doc = nlp(text)\n","\n","    entites_unique = []\n","    seen_entities = set()\n","\n","    for i, ent in enumerate(doc.ents):\n","        entity_key = (ent.text, ent.type)\n","        if entity_key not in seen_entities:\n","            entites_unique.append(get_ent_dict(ent))  # Stocke uniquement {\"mot\":..., \"type\":...}\n","            seen_entities.add(entity_key)\n","\n","    return entites_unique\n","\n","# Obtenir les étiquettes au format BIO\n","def bio_stanza(text: str, lang: str = \"fr\") -> list[str]:\n","    nlp = load_stanza_model(lang=lang)\n","    doc = nlp(text)\n","    return [\n","        [token.text, token.ner] for sentence in doc.sentences for token in sentence.tokens\n","    ]\n","\n","def remove_punctuation(token):\n","    return not re.match(r'[\\W_]+', token)\n","\n","if __name__ == \"__main__\":\n","    # Chemin du fichier d'entrée\n","    file_path = \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\"\n","    texte = lire_fichier(file_path)\n","\n","    # Extraire les entités nommées\n","    entites = dico_resultats(texte, lang=\"fr\")\n","    print(f\"Entités : {len(entites)}\")\n","    print(entites[:20])  # Affiche les 20 premières entités\n","\n","    # Afficher les tokens obtenus par Stanza\n","    nlp = load_stanza_model(lang=\"fr\")\n","    doc = nlp(texte)\n","    print(\"\\nTokens ：\")\n","    tokens = [token.text for sentence in doc.sentences for token in sentence.tokens]\n","    print(len(tokens))\n","    for token in tokens[:20]:\n","        print(token)\n","\n","    # Afficher les tokens non reconnus comme entité : candidats\n","    entite_mots = set(ent[\"mot\"] for ent in entites)\n","    candidats = []\n","\n","    for token in tokens:\n","        if token not in entite_mots and remove_punctuation(token):\n","            candidats.append(token)\n","\n","    print(\"\\nCandidats ：\")\n","    print(len(candidats))\n","    print(candidats[:20])  # Afficher les 20 premiers candidats\n"]},{"cell_type":"code","execution_count":null,"id":"b3ea2ea1-1652-44bb-8f5b-606a7cf36c50","metadata":{"scrolled":true,"colab":{"referenced_widgets":["3f52980bfdaf463cb42e67f347fcc04c"]},"id":"b3ea2ea1-1652-44bb-8f5b-606a7cf36c50","outputId":"2c9c0e5b-f601-4b86-ec6e-67fee1df2bcb"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-11 07:25:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f52980bfdaf463cb42e67f347fcc04c","version_major":2,"version_minor":0},"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2025-04-11 07:25:55 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n","2025-04-11 07:25:55 WARNING: Language fr package default expects mwt, which has been added\n","2025-04-11 07:25:55 INFO: Loading these models for language: fr (French):\n","==================================\n","| Processor | Package            |\n","----------------------------------\n","| tokenize  | combined           |\n","| mwt       | combined           |\n","| ner       | wikinergold_charlm |\n","==================================\n","\n","2025-04-11 07:25:55 INFO: Using device: cpu\n","2025-04-11 07:25:55 INFO: Loading: tokenize\n","2025-04-11 07:25:55 INFO: Loading: mwt\n","2025-04-11 07:25:55 INFO: Loading: ner\n","2025-04-11 07:25:57 INFO: Done loading processors!\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_stanza_model(lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# 处理整个语料库\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m all_entites, all_candidats, all_tokens \u001b[38;5;241m=\u001b[39m process_corpus(file_paths, nlp)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# 输出结果\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntités : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_entites)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[15], line 55\u001b[0m, in \u001b[0;36mprocess_corpus\u001b[0;34m(file_paths, nlp)\u001b[0m\n\u001b[1;32m     52\u001b[0m texte \u001b[38;5;241m=\u001b[39m lire_fichier(file_path)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# 获取命名实体\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m entites \u001b[38;5;241m=\u001b[39m dico_resultats(texte, nlp)\n\u001b[1;32m     56\u001b[0m all_entites\u001b[38;5;241m.\u001b[39mextend(entites)  \u001b[38;5;66;03m# 累积所有命名实体\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 获取 tokens\u001b[39;00m\n","Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mdico_resultats\u001b[0;34m(text, nlp)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdico_resultats\u001b[39m(text, nlp) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m     30\u001b[0m     entites_unique \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m     seen_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(doc, processors)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m process(doc)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stanza/pipeline/ner_processor.py:114\u001b[0m, in \u001b[0;36mNERProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    112\u001b[0m         preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[0;32m--> 114\u001b[0m             preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(b)\n\u001b[1;32m    115\u001b[0m         all_preds\u001b[38;5;241m.\u001b[39mappend(preds)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# for each sentence, gather a list of predictions\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# merge those predictions into a single list\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# earlier models will have precedence\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stanza/models/ner/trainer.py:139\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, batch, unsort)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m#batch_size = word.size(0)\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m _, logits, trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(word, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# decode\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# TODO: might need to decode multiple columns of output for\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# models with multiple layers\u001b[39;00m\n\u001b[1;32m    144\u001b[0m trans \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m trans]\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stanza/models/ner/model.py:211\u001b[0m, in \u001b[0;36mNERTagger.forward\u001b[0;34m(self, sentences, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[1;32m    209\u001b[0m char_reps_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharmodel_forward\u001b[38;5;241m.\u001b[39mget_representation(chars[\u001b[38;5;241m0\u001b[39m], charoffsets[\u001b[38;5;241m0\u001b[39m], charlens, char_orig_idx)\n\u001b[1;32m    210\u001b[0m char_reps_forward \u001b[38;5;241m=\u001b[39m PackedSequence(char_reps_forward\u001b[38;5;241m.\u001b[39mdata, char_reps_forward\u001b[38;5;241m.\u001b[39mbatch_sizes)\n\u001b[0;32m--> 211\u001b[0m char_reps_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharmodel_backward\u001b[38;5;241m.\u001b[39mget_representation(chars[\u001b[38;5;241m1\u001b[39m], charoffsets[\u001b[38;5;241m1\u001b[39m], charlens, char_orig_idx)\n\u001b[1;32m    212\u001b[0m char_reps_backward \u001b[38;5;241m=\u001b[39m PackedSequence(char_reps_backward\u001b[38;5;241m.\u001b[39mdata, char_reps_backward\u001b[38;5;241m.\u001b[39mbatch_sizes)\n\u001b[1;32m    213\u001b[0m inputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [char_reps_forward, char_reps_backward]\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stanza/models/common/char_model.py:160\u001b[0m, in \u001b[0;36mCharacterLanguageModel.get_representation\u001b[0;34m(self, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_representation\u001b[39m(\u001b[38;5;28mself\u001b[39m, chars, charoffsets, charlens, char_orig_idx):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 160\u001b[0m         output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(chars, charlens)\n\u001b[1;32m    161\u001b[0m         res \u001b[38;5;241m=\u001b[39m [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(charoffsets)]\n\u001b[1;32m    162\u001b[0m         res \u001b[38;5;241m=\u001b[39m unsort(res, char_orig_idx)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stanza/models/common/char_model.py:153\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[0;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m    151\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_h_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m    152\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_c_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m--> 153\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm(embs, charlens, hx\u001b[38;5;241m=\u001b[39mhidden)\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stanza/models/common/packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[0;34m(self, input, lengths, hx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad:\n\u001b[1;32m     24\u001b[0m     res \u001b[38;5;241m=\u001b[39m (pad_packed_sequence(res[\u001b[38;5;241m0\u001b[39m], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)[\u001b[38;5;241m0\u001b[39m], res[\u001b[38;5;241m1\u001b[39m])\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1136\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1126\u001b[0m         hx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1134\u001b[0m     )\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1138\u001b[0m         batch_sizes,\n\u001b[1;32m   1139\u001b[0m         hx,\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1146\u001b[0m     )\n\u001b[1;32m   1147\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1148\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import nltk\n","import stanza\n","import re\n","\n","\n","\n","\n","# Charger le modèle Stanza une fois pour复用\n","def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n","    nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    return nlp\n","\n","# Lire le texte à partir d'un fichier\n","def lire_fichier(chemin, is_json=False):\n","    with open(chemin, encoding='utf-8') as f:\n","        if is_json:\n","            return json.load(f)\n","        else:\n","            return f.read().strip()\n","\n","# Obtenir un dictionnaire représentant une entité nommée (modifié selon la structure demandée)\n","def get_ent_dict(ent) -> dict:\n","    return {\"mot\": ent.text, \"type\": ent.type}\n","\n","# Traiter le texte et retourner une liste d'entités sans doublons\n","def dico_resultats(text, nlp) -> dict:\n","    doc = nlp(text)\n","\n","    entites_unique = []\n","    seen_entities = set()\n","\n","    for i, ent in enumerate(doc.ents):\n","        entity_key = (ent.text, ent.type)\n","        if entity_key not in seen_entities:\n","            entites_unique.append(get_ent_dict(ent))  # Stocke uniquement {\"mot\":..., \"type\":...}\n","            seen_entities.add(entity_key)\n","\n","    return entites_unique\n","\n","# 去掉标点符号的函数\n","def remove_punctuation(token):\n","    return not re.match(r'[\\W_]+', token)  # 判断是否是标点符号，若是标点符号则返回 False\n","\n","# 处理整个corpus的函数\n","def process_corpus(file_paths, nlp):\n","    all_entites = []\n","    all_candidats = []\n","    all_tokens = []\n","\n","    for file_path in file_paths:\n","        texte = lire_fichier(file_path)\n","\n","        # 获取命名实体\n","        entites = dico_resultats(texte, nlp)\n","        all_entites.extend(entites)  # 累积所有命名实体\n","\n","        # 获取 tokens\n","        doc = nlp(texte)\n","        tokens = [token.text for sentence in doc.sentences for token in sentence.tokens]\n","        all_tokens.extend(tokens)  # 累积所有 tokens\n","\n","        # 获取未被识别为命名实体的词（candidats）\n","        entite_mots = set(ent[\"mot\"] for ent in entites)\n","        for token in tokens:\n","            if token not in entite_mots and remove_punctuation(token):  # 过滤标点符号\n","                all_candidats.append(token)  # 累积未识别的词\n","\n","    return all_entites, all_candidats, all_tokens\n","\n","if __name__ == \"__main__\":\n","    # 文件路径列表\n","    file_paths = [\n","        \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\",\n","        \"/Users/zhengruixing/Desktop/DARBOUVILLE_Poesies-et-nouvelles.txt\",\n","        \"/Users/zhengruixing/Desktop/DESBORDES-VALMORE_Poesies-1820.txt\",\n","        \"/Users/zhengruixing/Desktop/HUGO_Contemplations-T2.txt\",\n","        \"/Users/zhengruixing/Desktop/LOISEAU_Fleurs-d-avril.txt\",\n","        \"/Users/zhengruixing/Desktop/NOAILLES_Derniers-vers.txt\",\n","        \"/Users/zhengruixing/Desktop/RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine.txt\",\n","        \"/Users/zhengruixing/Desktop/VERLAINE_Sagesse.txt\",\n","        \"/Users/zhengruixing/Desktop/SAUVAGE_Tandis-que-la-terre-tourne.txt\",\n","        \"/Users/zhengruixing/Desktop/VIVIEN_Etudes-et-preludes.txt\"\n","    ]\n","\n","    # 先加载Stanza模型\n","    nlp = load_stanza_model(lang=\"fr\")\n","\n","    # 处理整个语料库\n","    all_entites, all_candidats, all_tokens = process_corpus(file_paths, nlp)\n","\n","    # 输出结果\n","    print(f\"Entités : {len(all_entites)}\")\n","    print(all_entites[:20])  # 打印前20个命名实体\n","\n","    print(f\"\\nCandidats : {len(all_candidats)}\")\n","    print(all_candidats[:20])  # 打印前20个未识别的词\n","\n","    print(f\"\\nTokens : {len(all_tokens)}\")\n","    print(all_tokens[:20])  # 打印前20个tokens\n"]},{"cell_type":"code","execution_count":null,"id":"dc7dcace-e9ba-4129-a2a5-0de6cf9fb6d7","metadata":{"colab":{"referenced_widgets":["acc2d427d406443f87fee51ae66cf3e6"]},"id":"dc7dcace-e9ba-4129-a2a5-0de6cf9fb6d7","outputId":"b4d8b334-9865-45a8-d232-2a014483dfc1"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-11 07:51:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"name":"stdout","output_type":"stream","text":["File paths to process:\n","/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\n","/Users/zhengruixing/Desktop/DARBOUVILLE_Poesies-et-nouvelles.txt\n","/Users/zhengruixing/Desktop/DESBORDES-VALMORE_Poesies-1820.txt\n","/Users/zhengruixing/Desktop/HUGO_Contemplations-T2.txt\n","/Users/zhengruixing/Desktop/LOISEAU_Fleurs-d-avril.txt\n","/Users/zhengruixing/Desktop/NOAILLES_Derniers-vers.txt\n","/Users/zhengruixing/Desktop/RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine.txt\n","/Users/zhengruixing/Desktop/VERLAINE_Sagesse.txt\n","/Users/zhengruixing/Desktop/SAUVAGE_Tandis-que-la-terre-tourne.txt\n","/Users/zhengruixing/Desktop/VIVIEN_Etudes-et-preludes.txt\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acc2d427d406443f87fee51ae66cf3e6","version_major":2,"version_minor":0},"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2025-04-11 07:51:43 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n","2025-04-11 07:51:43 WARNING: Language fr package default expects mwt, which has been added\n","2025-04-11 07:51:44 INFO: Loading these models for language: fr (French):\n","==================================\n","| Processor | Package            |\n","----------------------------------\n","| tokenize  | combined           |\n","| mwt       | combined           |\n","| ner       | wikinergold_charlm |\n","==================================\n","\n","2025-04-11 07:51:44 INFO: Using device: cpu\n","2025-04-11 07:51:44 INFO: Loading: tokenize\n","2025-04-11 07:51:44 INFO: Loading: mwt\n","2025-04-11 07:51:44 INFO: Loading: ner\n","2025-04-11 07:51:46 INFO: Done loading processors!\n"]},{"name":"stdout","output_type":"stream","text":["Entités : 8116\n","[{'mot': 'Calligrammes', 'type': 'MISC'}, {'mot': 'Guillaume Apollinaire', 'type': 'PER'}, {'mot': 'La Fresnaye', 'type': 'PER'}, {'mot': 'Apollinaire', 'type': 'PER'}, {'mot': 'Guillaume', 'type': 'PER'}, {'mot': 'Lausanne', 'type': 'LOC'}, {'mot': 'Roger de', 'type': 'PER'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Description : Collection : Collection du Bouquet', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'GUILLAUME APOLLINAIRE', 'type': 'PER'}, {'mot': 'CALLIGRAMMES', 'type': 'PER'}, {'mot': 'M E R M O D', 'type': 'LOC'}, {'mot': 'RENÉ DALIZE', 'type': 'PER'}]\n","\n","Candidats : 232577\n","['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages', '262', 'Notice', 'complète', 'Titre']\n","\n","Tokens : 296191\n","['Rappel', 'de', 'votre', 'demande', ':', 'Format', 'de', 'téléchargement', ':', ':', 'Texte', 'Vues', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages']\n"]}],"source":["import nltk\n","import stanza\n","import re\n","import glob\n","\n","# Charger le modèle Stanza une fois pour复用\n","def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n","    nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    return nlp\n","\n","# Lire le texte à partir d'un fichier\n","def lire_fichier(chemin, is_json=False):\n","    with open(chemin, encoding='utf-8') as f:\n","        if is_json:\n","            return json.load(f)\n","        else:\n","            return f.read().strip()\n","\n","# Obtenir un dictionnaire représentant une entité nommée (modifié selon la structure demandée)\n","def get_ent_dict(ent) -> dict:\n","    return {\"mot\": ent.text, \"type\": ent.type}\n","\n","# Traiter le texte et retourner une liste d'entités sans doublons\n","def dico_resultats(text, nlp) -> dict:\n","    doc = nlp(text)\n","\n","    entites_unique = []\n","    seen_entities = set()\n","\n","    for i, ent in enumerate(doc.ents):\n","        entity_key = (ent.text, ent.type)\n","        if entity_key not in seen_entities:\n","            entites_unique.append(get_ent_dict(ent))  # Stocke uniquement {\"mot\":..., \"type\":...}\n","            seen_entities.add(entity_key)\n","\n","    return entites_unique\n","\n","# 去掉标点符号的函数\n","def remove_punctuation(token):\n","    return not re.match(r'[\\W_]+', token)  # 判断是否是标点符号，若是标点符号则返回 False\n","\n","# 处理整个corpus的函数\n","def process_corpus(file_paths, nlp):\n","    all_entites = []\n","    all_candidats = []\n","    all_tokens = []\n","\n","    for file_path in file_paths:\n","        texte = lire_fichier(file_path)\n","\n","        # 获取命名实体\n","        entites = dico_resultats(texte, nlp)\n","        all_entites.extend(entites)  # 累积所有命名实体\n","\n","        # 获取 tokens\n","        doc = nlp(texte)\n","        tokens = [token.text for sentence in doc.sentences for token in sentence.tokens]\n","        all_tokens.extend(tokens)  # 累积所有 tokens\n","\n","        # 获取未被识别为命名实体的词（candidats）\n","        entite_mots = set(ent[\"mot\"] for ent in entites)\n","        for token in tokens:\n","            if token not in entite_mots and remove_punctuation(token):  # 过滤标点符号\n","                all_candidats.append(token)  # 累积未识别的词\n","\n","    return all_entites, all_candidats, all_tokens\n","\n","if __name__ == \"__main__\":\n","    # 手动列出的文件路径\n","    manual_file_paths = [\n","        \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\",\n","        \"/Users/zhengruixing/Desktop/DARBOUVILLE_Poesies-et-nouvelles.txt\",\n","        \"/Users/zhengruixing/Desktop/DESBORDES-VALMORE_Poesies-1820.txt\",\n","        \"/Users/zhengruixing/Desktop/HUGO_Contemplations-T2.txt\",\n","        \"/Users/zhengruixing/Desktop/LOISEAU_Fleurs-d-avril.txt\",\n","        \"/Users/zhengruixing/Desktop/NOAILLES_Derniers-vers.txt\",\n","        \"/Users/zhengruixing/Desktop/RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine.txt\",\n","        \"/Users/zhengruixing/Desktop/VERLAINE_Sagesse.txt\",\n","        \"/Users/zhengruixing/Desktop/SAUVAGE_Tandis-que-la-terre-tourne.txt\",\n","        \"/Users/zhengruixing/Desktop/VIVIEN_Etudes-et-preludes.txt\"\n","    ]\n","\n","    # 使用glob获取目录下的所有txt文件\n","    additional_file_paths = glob.glob(\"/Users/zhengruixing/Desktop/Corpus/*.txt\")\n","\n","    # 合并手动列出的路径和通过glob获取的路径\n","    file_paths = manual_file_paths + additional_file_paths\n","\n","    # 输出文件路径确认\n","    print(\"File paths to process:\")\n","    for path in file_paths:\n","        print(path)\n","\n","    # 先加载Stanza模型\n","    nlp = load_stanza_model(lang=\"fr\")\n","\n","    # 处理整个语料库\n","    all_entites, all_candidats, all_tokens = process_corpus(file_paths, nlp)\n","\n","    # 输出结果\n","    print(f\"Entités : {len(all_entites)}\")\n","    print(all_entites[:20])  # 打印前20个命名实体\n","\n","    print(f\"\\nCandidats : {len(all_candidats)}\")\n","    print(all_candidats[:20])  # 打印前20个未识别的词\n","\n","    print(f\"\\nTokens : {len(all_tokens)}\")\n","    print(all_tokens[:20])  # 打印前20个tokens\n","\n"]},{"cell_type":"code","execution_count":null,"id":"28b08b8d-f4d5-43db-8e7e-cdcf68761575","metadata":{"id":"28b08b8d-f4d5-43db-8e7e-cdcf68761575"},"outputs":[],"source":[" result = {\n","        \"entites\": all_entites,\n","        \"candidats\": all_candidats,\n","        \"tokens\": all_tokens\n","    }\n","\n","\n","with open(\"/Users/zhengruixing/Desktop/0411/tous_entites_nommes_Stanza.json\", \"w\", encoding=\"utf-8\") as json_file:\n","        json.dump(result, json_file, indent=4, ensure_ascii=False)"]},{"cell_type":"code","execution_count":null,"id":"46763593-71d6-46b5-8ef8-5a6393c1eac8","metadata":{"scrolled":true,"colab":{"referenced_widgets":["655b7e6057754a16b2231964fb3a6abf"]},"id":"46763593-71d6-46b5-8ef8-5a6393c1eac8","outputId":"6025be62-f7e5-4dff-b924-7da78d2e13a3"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-11 07:37:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"655b7e6057754a16b2231964fb3a6abf","version_major":2,"version_minor":0},"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2025-04-11 07:37:59 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n","2025-04-11 07:37:59 WARNING: Language fr package default expects mwt, which has been added\n","2025-04-11 07:38:00 INFO: Loading these models for language: fr (French):\n","==================================\n","| Processor | Package            |\n","----------------------------------\n","| tokenize  | combined           |\n","| mwt       | combined           |\n","| ner       | wikinergold_charlm |\n","==================================\n","\n","2025-04-11 07:38:00 INFO: Using device: cpu\n","2025-04-11 07:38:00 INFO: Loading: tokenize\n","2025-04-11 07:38:00 INFO: Loading: mwt\n","2025-04-11 07:38:00 INFO: Loading: ner\n","2025-04-11 07:38:02 INFO: Done loading processors!\n","Processing files:  10%|██▍                     | 1/10 [00:31<04:47, 31.99s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : APPOLINAIRE_Caligrammes\n","  Tokens: 16683\n","  Entités: 584\n","  Candidats: 16004\n","  Temps de traitement : 31.99 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Calligrammes', 'type': 'MISC'}, {'mot': 'Guillaume Apollinaire', 'type': 'PER'}, {'mot': 'La Fresnaye', 'type': 'PER'}, {'mot': 'Apollinaire', 'type': 'PER'}, {'mot': 'Guillaume', 'type': 'PER'}, {'mot': 'Lausanne', 'type': 'LOC'}, {'mot': 'Roger de', 'type': 'PER'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Description : Collection : Collection du Bouquet', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'GUILLAUME APOLLINAIRE', 'type': 'PER'}, {'mot': 'CALLIGRAMMES', 'type': 'PER'}, {'mot': 'M E R M O D', 'type': 'LOC'}, {'mot': 'RENÉ DALIZE', 'type': 'PER'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages', '262', 'Notice', 'complète', 'Titre']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files:  20%|████▊                   | 2/10 [02:10<09:31, 71.38s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : DARBOUVILLE_Poesies-et-nouvelles\n","  Tokens: 55462\n","  Entités: 1064\n","  Candidats: 42992\n","  Temps de traitement : 98.94 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Titre : Poésies et nouvelles', 'type': 'MISC'}, {'mot': \"Madame d'Arbouville\", 'type': 'PER'}, {'mot': \"Sophie d'Arbouville\", 'type': 'PER'}, {'mot': 'P. de Barante', 'type': 'PER'}, {'mot': 'Arbouville', 'type': 'PER'}, {'mot': 'Sophie', 'type': 'PER'}, {'mot': 'Amyot', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Barante', 'type': 'PER'}, {'mot': 'Prosper Brugière', 'type': 'PER'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'NOUVELLES', 'type': 'ORG'}, {'mot': 'TYPOGRAPHIE DE CH. LAHURE Imprimeur du Sénat', 'type': 'ORG'}, {'mot': 'Cour de Cassation rue de Vaugirard', 'type': 'ORG'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '410', 'sur', '410', 'Nombre', 'de', 'pages', '410', 'Notice', 'complète', 'Titre']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files:  30%|███████▏                | 3/10 [03:03<07:19, 62.72s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : DESBORDES-VALMORE_Poesies-1820\n","  Tokens: 30825\n","  Entités: 905\n","  Candidats: 23758\n","  Temps de traitement : 52.42 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Poésies', 'type': 'MISC'}, {'mot': 'Mme Desbordes-Valmore', 'type': 'PER'}, {'mot': 'Desbordes-Valmore', 'type': 'PER'}, {'mot': 'Marceline', 'type': 'PER'}, {'mot': 'F. Louis', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Réserve des livres rares', 'type': 'LOC'}, {'mot': 'RES P-YE-785', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'Gars', 'type': 'LOC'}, {'mot': 'FRANÇOIS LOUIS', 'type': 'PER'}, {'mot': 'G1E S', 'type': 'LOC'}, {'mot': 'ELEGIES', 'type': 'ORG'}, {'mot': 'VVV', 'type': 'MISC'}, {'mot': 'T', 'type': 'MISC'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '213', 'sur', '213', 'Nombre', 'de', 'pages', '213', 'Notice', 'complète', 'Titre']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files:  40%|█████████▌              | 4/10 [05:01<08:28, 84.77s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : HUGO_Contemplations-T2\n","  Tokens: 65385\n","  Entités: 1531\n","  Candidats: 47156\n","  Temps de traitement : 118.55 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Titre : Les contemplations', 'type': 'MISC'}, {'mot': 'Victor Hugo', 'type': 'PER'}, {'mot': 'Hugo', 'type': 'PER'}, {'mot': 'Victor', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'LES', 'type': 'ORG'}, {'mot': 'CONTEMPLATIONS', 'type': 'PER'}, {'mot': 'Ai!', 'type': 'MISC'}, {'mot': 'VICTOR HUGO', 'type': 'PER'}, {'mot': 'TOME li', 'type': 'PER'}, {'mot': 'Jk i J', 'type': 'MISC'}, {'mot': 'D I R. B', 'type': 'MISC'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '246', 'sur', '246', 'Nombre', 'de', 'pages', '246', 'Notice', 'complète', 'Titre']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files:  50%|████████████            | 5/10 [05:40<05:41, 68.27s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : LOISEAU_Fleurs-d-avril\n","  Tokens: 20602\n","  Entités: 538\n","  Candidats: 16336\n","  Temps de traitement : 39.03 secondes\n","--------------------\n","  20 premières entités : [{'mot': \"Titre : Fleurs d'avril\", 'type': 'MISC'}, {'mot': 'Jeanne Loiseau', 'type': 'PER'}, {'mot': 'Loiseau', 'type': 'PER'}, {'mot': 'Jeanne', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': '8-YE-137', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'JEANNE LOISEAU', 'type': 'PER'}, {'mot': 'tVi', 'type': 'ORG'}, {'mot': 'Fleurs d’Avril', 'type': 'MISC'}, {'mot': 'POESIES', 'type': 'ORG'}, {'mot': 'SŸi', 'type': 'PER'}, {'mot': 'I H K l', 'type': 'LOC'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '136', 'sur', '136', 'Nombre', 'de', 'pages', '136', 'Notice', 'complète', 'Titre']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files:  60%|██████████████▍         | 6/10 [06:09<03:39, 54.75s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : NOAILLES_Derniers-vers\n","  Tokens: 12820\n","  Entités: 479\n","  Candidats: 10431\n","  Temps de traitement : 28.49 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Titre : Derniers vers', 'type': 'MISC'}, {'mot': 'Comtesse de Noailles', 'type': 'PER'}, {'mot': 'Noailles', 'type': 'PER'}, {'mot': 'Anna', 'type': 'PER'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Réserve des livres rares', 'type': 'LOC'}, {'mot': 'RES G-YE-122', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'COMTESSE DE NOAILLES', 'type': 'PER'}, {'mot': 'GRASSET', 'type': 'PER'}, {'mot': 'DERNIERS', 'type': 'ORG'}, {'mot': 'ANNE-JULES & HÉLÈNE', 'type': 'ORG'}, {'mot': 'Constantin PHOTIADÈS', 'type': 'PER'}, {'mot': 'AVERTISSEMENT', 'type': 'ORG'}, {'mot': 'comtesse de Noailles', 'type': 'PER'}, {'mot': 'Poème de P amour', 'type': 'MISC'}, {'mot': 'V Honneur', 'type': 'MISC'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'Format', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '166', 'sur', '166', 'Nombre', 'de', 'pages', '166', 'Notice', 'complète']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files:  70%|████████████████▊       | 7/10 [06:53<02:33, 51.16s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine\n","  Tokens: 20685\n","  Entités: 463\n","  Candidats: 16622\n","  Temps de traitement : 43.77 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Titre : les Illuminations', 'type': 'MISC'}, {'mot': 'Une Saison en Enfer', 'type': 'MISC'}, {'mot': 'Paul Verlaine', 'type': 'PER'}, {'mot': 'Titre : Une Saison en Enfer', 'type': 'MISC'}, {'mot': 'Rimbaud', 'type': 'PER'}, {'mot': 'Arthur', 'type': 'PER'}, {'mot': 'L. Vanier', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Verlaine', 'type': 'PER'}, {'mot': 'Paul', 'type': 'PER'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Réserve des livres rares', 'type': 'LOC'}, {'mot': 'RESP-Z-2180', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'UNE SAISON', 'type': 'MISC'}, {'mot': 'ENFER', 'type': 'ORG'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'Format', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '169', 'sur', '169', 'Nombre', 'de', 'pages', '169', 'Notice', 'complète']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files:  80%|███████████████████▏    | 8/10 [07:20<01:27, 43.54s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : VERLAINE_Sagesse\n","  Tokens: 14122\n","  Entités: 603\n","  Candidats: 11213\n","  Temps de traitement : 27.22 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Titre : Sagesse', 'type': 'MISC'}, {'mot': 'Paul Verlaine', 'type': 'PER'}, {'mot': 'Verlaine', 'type': 'PER'}, {'mot': 'Paul', 'type': 'PER'}, {'mot': 'L. Vanier', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Sagesse', 'type': 'PER'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'PAUL VERLAINE', 'type': 'PER'}, {'mot': 'SAGESSE', 'type': 'ORG'}, {'mot': 'REVUE El CORUIUKE', 'type': 'ORG'}, {'mot': 'PARIS', 'type': 'ORG'}, {'mot': 'EH', 'type': 'ORG'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '167', 'sur', '167', 'Nombre', 'de', 'pages', '167', 'Notice', 'complète', 'Titre']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files:  90%|█████████████████████▌  | 9/10 [08:02<00:42, 42.98s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : SAUVAGE_Tandis-que-la-terre-tourne\n","  Tokens: 20726\n","  Entités: 481\n","  Candidats: 18190\n","  Temps de traitement : 41.73 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Titre : Tandis que la terre', 'type': 'MISC'}, {'mot': 'Cécile Sauvage', 'type': 'PER'}, {'mot': 'Sauvage', 'type': 'PER'}, {'mot': 'Cécile', 'type': 'PER'}, {'mot': 'Mercure de France', 'type': 'MISC'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'CÉCILE SAUVAGE', 'type': 'PER'}, {'mot': 'POEMES', 'type': 'MISC'}, {'mot': 'PARIS MERCVRE DE FRANGE XXVI', 'type': 'ORG'}, {'mot': 'RVE DE CONDÉ', 'type': 'ORG'}, {'mot': 'XXVI', 'type': 'ORG'}, {'mot': 'Hollande', 'type': 'PER'}, {'mot': 'TIRAGE', 'type': 'ORG'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '204', 'sur', '204', 'Nombre', 'de', 'pages', '204', 'Notice', 'complète', 'Titre']\n","========================================\n"]},{"name":"stderr","output_type":"stream","text":["Processing files: 100%|███████████████████████| 10/10 [09:17<00:00, 55.75s/file]"]},{"name":"stdout","output_type":"stream","text":["Fichier : VIVIEN_Etudes-et-preludes\n","  Tokens: 38881\n","  Entités: 1468\n","  Candidats: 29875\n","  Temps de traitement : 75.34 secondes\n","--------------------\n","  20 premières entités : [{'mot': 'Titre : Poèmes de Renée Vivien : Études et préludes', 'type': 'MISC'}, {'mot': 'Cendres et poussières', 'type': 'MISC'}, {'mot': 'Évocations', 'type': 'MISC'}, {'mot': 'Sapho', 'type': 'PER'}, {'mot': 'La Vénus des aveugles', 'type': 'MISC'}, {'mot': 'Vivien', 'type': 'PER'}, {'mot': 'Renée', 'type': 'PER'}, {'mot': 'A. Lemerre', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'XML DTBook', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'POÈMES', 'type': 'MISC'}, {'mot': 'D F', 'type': 'MISC'}]\n","  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '270', 'sur', '270', 'Nombre', 'de', 'pages', '270', 'Notice', 'complète', 'Titre']\n","========================================\n","Traitement terminé pour tous les fichiers.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import nltk\n","import stanza\n","import re\n","import json\n","import os\n","from tqdm import tqdm  # Importer la bibliothèque tqdm\n","import time  # Pour calculer le temps de traitement de chaque fichier\n","\n","# Charger le modèle Stanza une fois pour réutilisation\n","def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n","    nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n","    return nlp\n","\n","# Lire le texte à partir d'un fichier\n","def lire_fichier(chemin, is_json=False):\n","    with open(chemin, encoding='utf-8') as f:\n","        if is_json:\n","            return json.load(f)\n","        else:\n","            return f.read().strip()\n","\n","# Obtenir un dictionnaire représentant une entité nommée (modifié selon la structure demandée)\n","def get_ent_dict(ent) -> dict:\n","    return {\"mot\": ent.text, \"type\": ent.type}\n","\n","# Traiter le texte et retourner une liste d'entités sans doublons\n","def dico_resultats(text, nlp) -> dict:\n","    doc = nlp(text)\n","\n","    entites_unique = []\n","    seen_entities = set()\n","\n","    for i, ent in enumerate(doc.ents):\n","        entity_key = (ent.text, ent.type)\n","        if entity_key not in seen_entities:\n","            entites_unique.append(get_ent_dict(ent))  # Stocke uniquement {\"mot\":..., \"type\":...}\n","            seen_entities.add(entity_key)\n","\n","    return entites_unique\n","\n","# Fonction pour supprimer la ponctuation\n","def remove_punctuation(token):\n","    return not re.match(r'[\\W_]+', token)  # Vérifie si c'est un signe de ponctuation, retourne False si c'est le cas\n","\n","# Traiter l'ensemble du corpus\n","def process_corpus(file_paths, output_dir, nlp):\n","    os.makedirs(output_dir, exist_ok=True)  # Créer le répertoire de sortie si il n'existe pas\n","\n","    # Utiliser tqdm pour afficher une barre de progression\n","    for file_path in tqdm(file_paths, desc=\"Traitement des fichiers\", unit=\"fichier\"):\n","        start_time = time.time()  # Enregistrer l'heure de début du traitement\n","\n","        # Obtenir le nom du fichier\n","        file_name = os.path.basename(file_path).split('.')[0]\n","\n","        # Lire le contenu du fichier\n","        texte = lire_fichier(file_path)\n","\n","        # Extraire les entités nommées\n","        entites = dico_resultats(texte, nlp)\n","\n","        # Extraire les tokens\n","        doc = nlp(texte)\n","        tokens = [token.text for sentence in doc.sentences for token in sentence.tokens]\n","\n","        # Extraire les mots qui ne sont pas des entités nommées (candidats)\n","        entite_mots = set(ent[\"mot\"] for ent in entites)\n","        candidats = [token for token in tokens if token not in entite_mots and remove_punctuation(token)]\n","\n","        # Générer les données de sortie\n","        output_data = {\n","            \"entites_nom\": entites,\n","            \"candidats\": candidats\n","        }\n","\n","        # Sauvegarder les résultats dans un fichier JSON\n","        json_file_path = os.path.join(output_dir, f\"{file_name}_entites_candidats.json\")\n","        with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n","            json.dump(output_data, json_file, indent=4, ensure_ascii=False)\n","\n","        # Calculer et afficher le temps de traitement pour chaque fichier\n","        processing_time = time.time() - start_time\n","        print(f\"Fichier : {file_name}\")\n","        print(f\"  Tokens: {len(tokens)}\")\n","        print(f\"  Entités: {len(entites)}\")\n","        print(f\"  Candidats: {len(candidats)}\")\n","        print(f\"  Temps de traitement : {processing_time:.2f} secondes\")\n","        print(\"-\" * 20)\n","\n","        # Afficher les 20 premières entités et candidats\n","        print(f\"  20 premières entités : {entites[:20]}\")\n","        print(f\"  20 premiers candidats : {candidats[:20]}\")\n","        print(\"=\" * 40)\n","\n","        # Sauvegarder les entités dans un fichier texte\n","        with open(os.path.join(output_dir, f\"{file_name}_entites.txt\"), \"w\", encoding=\"utf-8\") as entites_file:\n","            for entite in entites:\n","                entites_file.write(f\"{entite['mot']} - {entite['type']}\\n\")\n","\n","        # Sauvegarder les candidats dans un fichier texte\n","        with open(os.path.join(output_dir, f\"{file_name}_candidats.txt\"), \"w\", encoding=\"utf-8\") as candidats_file:\n","            for mot in candidats:\n","                candidats_file.write(f\"{mot}\\n\")\n","\n","    # Afficher un message lorsque tous les fichiers sont traités\n","    print(\"Traitement terminé pour tous les fichiers.\")\n","\n","if __name__ == \"__main__\":\n","    # Liste des chemins des fichiers\n","    file_paths = [\n","        \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\",\n","        \"/Users/zhengruixing/Desktop/DARBOUVILLE_Poesies-et-nouvelles.txt\",\n","        \"/Users/zhengruixing/Desktop/DESBORDES-VALMORE_Poesies-1820.txt\",\n","        \"/Users/zhengruixing/Desktop/HUGO_Contemplations-T2.txt\",\n","        \"/Users/zhengruixing/Desktop/LOISEAU_Fleurs-d-avril.txt\",\n","        \"/Users/zhengruixing/Desktop/NOAILLES_Derniers-vers.txt\",\n","        \"/Users/zhengruixing/Desktop/RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine.txt\",\n","        \"/Users/zhengruixing/Desktop/VERLAINE_Sagesse.txt\",\n","        \"/Users/zhengruixing/Desktop/SAUVAGE_Tandis-que-la-terre-tourne.txt\",\n","        \"/Users/zhengruixing/Desktop/VIVIEN_Etudes-et-preludes.txt\"\n","    ]\n","\n","    # Répertoire de sortie\n","    output_dir = \"/Users/zhengruixing/Desktop/0411/Stanza_output\"\n","\n","    # Charger le modèle Stanza\n","    nlp = load_stanza_model(lang=\"fr\")\n","\n","    # Traiter l'ensemble du corpus et sauvegarder les résultats\n","    process_corpus(file_paths, output_dir, nlp)\n"]},{"cell_type":"markdown","id":"9de98014-fd83-4b86-9588-faf6f9296251","metadata":{"id":"9de98014-fd83-4b86-9588-faf6f9296251"},"source":["### Exclure les limitations de Stanza"]},{"cell_type":"markdown","id":"50841f25-afdd-4edc-bf0b-ec215fc43ca1","metadata":{"id":"50841f25-afdd-4edc-bf0b-ec215fc43ca1"},"source":["Stanza utilise effectivement des modèles basés sur des architectures similaires à BERT (comme celles du cadre transformers), et ces modèles peuvent être soumis à une limitation de la longueur d'entrée, ce qui signifie que les textes plus longs risquent de dépasser cette limite. En conséquence, Stanza pourrait être amené à diviser les textes en segments plus petits afin de respecter cette limite de longueur d'entrée.\n","\n","Cependant, Stanza ne fournit pas de documentation précise sur la longueur maximale de chaque entrée pour chaque modèle qu'il utilise. En règle générale, cela dépend de l'implémentation du modèle sous-jacent. Pour la plupart des modèles, cette gestion est automatisée, ce qui signifie que Stanza s'occupe de diviser le texte en segments appropriés de manière interne, sans que l'utilisateur ait à se soucier de cette question. Cela garantit que le texte ne dépasse pas la longueur maximale autorisée par le modèle sous-jacent.\n","\n","Stanza 在其处理过程中确实会利用类似于 BERT 的模型（如 transformers 库的实现），因此也可能面临 最大输入长度 的限制。这意味着，在处理较长的文本时，Stanza 也可能将文本切分成较小的片段进行处理，以适应最大输入长度的要求。\n","\n","但是，Stanza 并没有明确的文档说明对每个输入句子的长度限制是多少，通常这依赖于底层模型的具体实现。大多数情况下，Stanza 会在其底层模型的基础上自动处理这一点，所以开发者不需要显式地拆分句子。其文本处理过程会进行适当的拆分，以避免超出模型的输入长度限制。"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
