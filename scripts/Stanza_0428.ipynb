{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b882bc49-e701-47b6-9809-84af4747b0cc",
   "metadata": {},
   "source": [
    "### Ici, \"Candidats\" : ce sont des mots ordinaires qui n'ont pas été identifiés comme entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae90f3e2-42f2-42a8-8b53-d0f755580e7f",
   "metadata": {
    "id": "ae90f3e2-42f2-42a8-8b53-d0f755580e7f",
    "outputId": "0f696a6d-8ce3-4544-885b-1cc9c2c1dae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4ba2f-1ea3-44b2-a0da-a6043f5ff179",
   "metadata": {},
   "source": [
    "### Exemple de REN en utilisant Stanford pour une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7105544-63ba-4fbd-9241-5ddad4ebd64b",
   "metadata": {
    "id": "1099ca0f-4296-4716-8472-bf7f7d0e1386",
    "outputId": "6fd50278-3eea-4c53-bc9c-db81eaa87268"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "# Chemin du modèle Stanford NER\n",
    "#stanford_classifier = \"stanford-ner-4.2.0/classifiers/english.all.3class.distsim.crf.ser.gz\"\n",
    "stanford_classifier = \"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz\"\n",
    "#stanford_classifier = \"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/classifiers/french-ner.ser.gz\"  # Modèle français\n",
    "stanford_ner_path =\"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/stanford-ner.jar\"\n",
    "\n",
    "# Créer le tagger NER\n",
    "ner_tagger = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "\n",
    "# Texte d'exemple\n",
    "text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n",
    "\n",
    "# Tokenisation\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Effectuer l'étiquetage NER\n",
    "ner_results = ner_tagger.tag(words)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83950134-fd18-41c3-909d-bd79966a5f6a",
   "metadata": {
    "id": "83950134-fd18-41c3-909d-bd79966a5f6a"
   },
   "source": [
    "Le modèle Stanford NER génère des résultats d'étiquetage des entités nommées (NER) pour le texte. Chaque tuple contient deux éléments : un mot (ou un symbole) et le type d'entité nommée associé à ce mot.\n",
    "\n",
    "PERSON indique un nom de personne (comme 'Barack' et 'Obama').\n",
    "LOCATION indique un nom de lieu (comme 'Hawaii' et 'United States').\n",
    "O indique une catégorie \"autre\", ce qui signifie que ce mot n'est pas une entité nommée ou qu'il ne correspond à aucune catégorie spécifique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cae71-829c-4293-9e75-2c5030d7e662",
   "metadata": {},
   "source": [
    "### Exemple de REN en utilisant Stanford pour un ouvrage dans nôtre corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61634624-53da-46d1-bccf-b24f08e04038",
   "metadata": {
    "id": "a1c8ea2e-d199-462c-8c35-988d0a4d151d",
    "outputId": "66a93555-e39a-4142-af11-a231c0203fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités   : 329\n",
      "Candidats : 15393\n",
      "[{'mot': 'Texte', 'type': 'ORGANIZATION'}, {'mot': 'Vues', 'type': 'ORGANIZATION'}, {'mot': 'Calligrammes', 'type': 'ORGANIZATION'}, {'mot': '', 'type': 'ORGANIZATION'}, {'mot': 'Guillaume', 'type': 'ORGANIZATION'}, {'mot': 'Apollinaire', 'type': 'ORGANIZATION'}, {'mot': 'Guillaume', 'type': 'PERSON'}, {'mot': 'Lausanne', 'type': 'LOCATION'}, {'mot': 'Roger', 'type': 'PERSON'}, {'mot': 'France', 'type': 'LOCATION'}, {'mot': 'Il', 'type': 'PERSON'}, {'mot': 'GUILLAUME', 'type': 'PERSON'}, {'mot': 'APOLLINAIRE', 'type': 'PERSON'}, {'mot': 'Cordes', 'type': 'PERSON'}, {'mot': 'Cordes', 'type': 'PERSON'}, {'mot': 'Cordes', 'type': 'PERSON'}, {'mot': 'Cordes', 'type': 'PERSON'}, {'mot': 'Bigorneaux', 'type': 'LOCATION'}, {'mot': 'Lotte', 'type': 'LOCATION'}, {'mot': 'Puits', 'type': 'PERSON'}]\n",
      "--------------------------------------------------\n",
      "['Rappel', 'de', 'votre', 'demande', ':', 'Format', 'de', 'téléchargement', ':', ':', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages', ':', '262']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import re\n",
    "\n",
    "\n",
    "stanford_classifier = \"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz\"\n",
    "stanford_ner_path = \"/Users/zhengruixing/Desktop/stanford-ner-2020-11-17/stanford-ner.jar\"\n",
    "\n",
    "\n",
    "ner_tagger = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "\n",
    "with open(\"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texte = f.read().strip()\n",
    "\n",
    "liste_entitesnom = []\n",
    "liste_candidat = []\n",
    "\n",
    "words = nltk.word_tokenize(texte)\n",
    "ner_results = ner_tagger.tag(words)\n",
    "for word, tag in ner_results:\n",
    "    if tag != \"O\":\n",
    "        liste_entitesnom.append({\"mot\": word, \"type\": tag})\n",
    "    else:\n",
    "        liste_candidat.append(word)\n",
    "\n",
    "print(f\"Entités   : {len(liste_entitesnom)}\")\n",
    "print(f\"Candidats : {len(liste_candidat )}\")\n",
    "print(liste_entitesnom[:20])\n",
    "print(\"-\"*50)\n",
    "print(list(liste_candidat )[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77922e2-7d0f-4b87-8a03-0dbf63b4d1f9",
   "metadata": {
    "id": "a77922e2-7d0f-4b87-8a03-0dbf63b4d1f9"
   },
   "source": [
    "Étant donné que Stanford NER n'a pas de modèle de traitement officiel pour le français, le modèle utilisé actuellement est celui en anglais. Cependant, selon les résultats d'annotation, le modèle en anglais génère de nombreuses erreurs de marquage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97414e40-da5f-4cdc-82b8-70412746da08",
   "metadata": {
    "id": "f25aefc9-e119-44a9-81ea-7ba830fceefb"
   },
   "source": [
    "### Stanza est un package de traitement du langage naturel développé par l'Université de Stanford, offrant un support multilingue basé sur l'apprentissage profond, y compris pour le français. Nous pouvons utiliser la bibliothèque Stanza pour effectuer la reconnaissance d'entités nommées. https://github.com/These-SCAI2023/CORPUS/blob/master/prog/Use_stanza.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e54630-d3ef-42d5-91e2-ef5bbf33ee7e",
   "metadata": {
    "id": "70e54630-d3ef-42d5-91e2-ef5bbf33ee7e"
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n",
    "    try:\n",
    "        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n",
    "    except:\n",
    "        stanza.download(lang=lang, logging_level='DEBUG')\n",
    "        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a69b6-fe5b-439e-a42e-6c204b5dac4c",
   "metadata": {},
   "source": [
    "#### Exemple de REN en utilisant Stanza pour un ouvrage dans nôtre corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25c59a-bf74-4032-b00c-a237b44d66b3",
   "metadata": {},
   "source": [
    "##### L'accent est mis sur les entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e29d915d-928c-4f76-95a8-ad4fb2fde0eb",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6d0cd9840af4450c818f55b5e801376a"
     ]
    },
    "id": "e29d915d-928c-4f76-95a8-ad4fb2fde0eb",
    "outputId": "994a4bcb-27f4-434e-e2f0-649cdcb35679",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:41:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf73736ce9a4faeb642817a67cefd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:41:31 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n",
      "2025-04-27 15:41:31 WARNING: Language fr package default expects mwt, which has been added\n",
      "2025-04-27 15:41:31 INFO: Loading these models for language: fr (French):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | combined           |\n",
      "| mwt       | combined           |\n",
      "| ner       | wikinergold_charlm |\n",
      "==================================\n",
      "\n",
      "2025-04-27 15:41:31 INFO: Using device: cpu\n",
      "2025-04-27 15:41:31 INFO: Loading: tokenize\n",
      "2025-04-27 15:41:31 INFO: Loading: mwt\n",
      "2025-04-27 15:41:31 INFO: Loading: ner\n",
      "2025-04-27 15:41:33 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités : 584\n",
      "[{'entite_0': {'label': 'MISC', 'text': 'Calligrammes', 'jalons': [132, 144]}}, {'entite_1': {'label': 'PER', 'text': 'Guillaume Apollinaire', 'jalons': [147, 168]}}, {'entite_2': {'label': 'PER', 'text': 'La Fresnaye', 'jalons': [182, 193]}}, {'entite_3': {'label': 'PER', 'text': 'Apollinaire', 'jalons': [204, 215]}}, {'entite_4': {'label': 'PER', 'text': 'Guillaume', 'jalons': [217, 226]}}, {'entite_5': {'label': 'LOC', 'text': 'Lausanne', 'jalons': [268, 276]}}, {'entite_7': {'label': 'PER', 'text': 'Roger de', 'jalons': [330, 338]}}, {'entite_8': {'label': 'MISC', 'text': 'Langue : Français', 'jalons': [550, 567]}}, {'entite_9': {'label': 'MISC', 'text': 'Format', 'jalons': [569, 575]}}, {'entite_11': {'label': 'MISC', 'text': 'Description : Collection : Collection du Bouquet', 'jalons': [699, 747]}}, {'entite_13': {'label': 'MISC', 'text': 'Droits : Public domain', 'jalons': [918, 940]}}, {'entite_14': {'label': 'MISC', 'text': 'Identifiant', 'jalons': [942, 953]}}, {'entite_15': {'label': 'LOC', 'text': 'Bibliothèque nationale de France', 'jalons': [991, 1023]}}, {'entite_16': {'label': 'ORG', 'text': 'Littérature et art', 'jalons': [1037, 1055]}}, {'entite_17': {'label': 'MISC', 'text': 'Conservation numérique', 'jalons': [1072, 1094]}}, {'entite_19': {'label': 'MISC', 'text': 'OCR', 'jalons': [1359, 1362]}}, {'entite_20': {'label': 'PER', 'text': 'GUILLAUME APOLLINAIRE', 'jalons': [1432, 1453]}}, {'entite_21': {'label': 'PER', 'text': 'CALLIGRAMMES', 'jalons': [1459, 1471]}}, {'entite_23': {'label': 'LOC', 'text': 'M E R M O D', 'jalons': [1497, 1508]}}, {'entite_28': {'label': 'PER', 'text': 'RENÉ DALIZE', 'jalons': [1642, 1653]}}]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Charger le modèle Stanza\n",
    "def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n",
    "    try:\n",
    "        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n",
    "    except:\n",
    "        stanza.download(lang=lang, logging_level='DEBUG')\n",
    "        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n",
    "    return nlp\n",
    "\n",
    "# Lire le texte à partir d'un fichier\n",
    "def lire_fichier(chemin, is_json=False):\n",
    "    with open(chemin, encoding='utf-8') as f:\n",
    "        if is_json:\n",
    "            return json.load(f)\n",
    "        else:\n",
    "            return f.read().strip()\n",
    "\n",
    "# Obtenir un dictionnaire représentant une entité nommée\n",
    "def get_ent_dict(ent) -> dict:\n",
    "    return {\"label\": ent.type, \"text\": ent.text, \"jalons\": [ent.start_char, ent.end_char]}\n",
    "\n",
    "# Traiter le texte et retourner un dictionnaire des entités\n",
    "def dico_resultats(text, lang: str = \"fr\") -> dict:\n",
    "    nlp = load_stanza_model(lang=lang)\n",
    "    doc = nlp(text)\n",
    "    # Éviter les doublons en se basant sur le texte et le type\n",
    "    entites_unique = []\n",
    "    seen_entities = set()  # Pour suivre les entités déjà traitées\n",
    "\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        entity_key = (ent.text, ent.type)\n",
    "        if entity_key not in seen_entities:\n",
    "            entites_unique.append({f\"entite_{i}\": get_ent_dict(ent)})\n",
    "            seen_entities.add(entity_key)\n",
    "\n",
    "    return entites_unique\n",
    "\n",
    "# Obtenir le format d’étiquetage BIO\n",
    "def bio_stanza(text: str, lang: str = \"fr\") -> list[str]:\n",
    "    nlp = load_stanza_model(lang=lang)\n",
    "    doc = nlp(text)\n",
    "    return [\n",
    "        [token.text, token.ner] for sentence in doc.sentences for token in sentence.tokens\n",
    "    ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Chemin du fichier d'entrée\n",
    "    file_path = \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\"  # Veuillez le modifier selon votre chemin réel\n",
    "    texte = lire_fichier(file_path)\n",
    "\n",
    "    # Obtenir les entités nommées\n",
    "    entites = dico_resultats(texte, lang=\"fr\")\n",
    "    print(f\"Entités : {len(entites)}\")\n",
    "    print(entites[:20])  # Afficher les 20 premières entités\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "EBqk8kUo9zkN",
   "metadata": {
    "id": "EBqk8kUo9zkN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les entités nommées ont été sauvegardées dans : /Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes_entites.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "output_json_path = \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes_entites.json\"\n",
    "\n",
    "# Sauvegarder les entités dans un fichier JSON\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(entites, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Les entités nommées ont été sauvegardées dans : {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d917d-bb58-4db1-9608-45d1ca91017c",
   "metadata": {},
   "source": [
    "#### Exemple de REN en utilisant Stanza pour un ouvrage dans nôtre corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979d156-af57-47a5-bf24-390940dcb5c5",
   "metadata": {},
   "source": [
    "##### L'accent est mis sur le nombre de tokens obtenus avec Stanza et sur candidats （les mots courants non reconnus comme entités nommées）.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ece07bc4-173f-43c0-9fb6-81f04404f7ad",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "055501db64074e90b7b965f3c49f366c",
      "d3719cbe5f9b4d848ae98a47b76e81af"
     ]
    },
    "id": "ece07bc4-173f-43c0-9fb6-81f04404f7ad",
    "outputId": "50387fc7-0545-4548-809a-2416f1ee9b1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:42:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3523303f18b04dbc9e8aae3d50daa392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:42:08 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n",
      "2025-04-27 15:42:08 WARNING: Language fr package default expects mwt, which has been added\n",
      "2025-04-27 15:42:08 INFO: Loading these models for language: fr (French):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | combined           |\n",
      "| mwt       | combined           |\n",
      "| ner       | wikinergold_charlm |\n",
      "==================================\n",
      "\n",
      "2025-04-27 15:42:08 INFO: Using device: cpu\n",
      "2025-04-27 15:42:08 INFO: Loading: tokenize\n",
      "2025-04-27 15:42:08 INFO: Loading: mwt\n",
      "2025-04-27 15:42:08 INFO: Loading: ner\n",
      "2025-04-27 15:42:10 INFO: Done loading processors!\n",
      "2025-04-27 15:42:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités : 584\n",
      "[{'mot': 'Calligrammes', 'type': 'MISC'}, {'mot': 'Guillaume Apollinaire', 'type': 'PER'}, {'mot': 'La Fresnaye', 'type': 'PER'}, {'mot': 'Apollinaire', 'type': 'PER'}, {'mot': 'Guillaume', 'type': 'PER'}, {'mot': 'Lausanne', 'type': 'LOC'}, {'mot': 'Roger de', 'type': 'PER'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Description : Collection : Collection du Bouquet', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'GUILLAUME APOLLINAIRE', 'type': 'PER'}, {'mot': 'CALLIGRAMMES', 'type': 'PER'}, {'mot': 'M E R M O D', 'type': 'LOC'}, {'mot': 'RENÉ DALIZE', 'type': 'PER'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3c4d9b1e7c4ce3913c2f821fa84a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:42:26 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n",
      "2025-04-27 15:42:26 WARNING: Language fr package default expects mwt, which has been added\n",
      "2025-04-27 15:42:26 INFO: Loading these models for language: fr (French):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | combined           |\n",
      "| mwt       | combined           |\n",
      "| ner       | wikinergold_charlm |\n",
      "==================================\n",
      "\n",
      "2025-04-27 15:42:26 INFO: Using device: cpu\n",
      "2025-04-27 15:42:26 INFO: Loading: tokenize\n",
      "2025-04-27 15:42:26 INFO: Loading: mwt\n",
      "2025-04-27 15:42:26 INFO: Loading: ner\n",
      "2025-04-27 15:42:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens ：\n",
      "16683\n",
      "Rappel\n",
      "de\n",
      "votre\n",
      "demande\n",
      ":\n",
      "Format\n",
      "de\n",
      "téléchargement\n",
      ":\n",
      ":\n",
      "Texte\n",
      "Vues\n",
      "1\n",
      "à\n",
      "262\n",
      "sur\n",
      "262\n",
      "Nombre\n",
      "de\n",
      "pages\n",
      "\n",
      "Candidats ：\n",
      "16004\n",
      "['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages', '262', 'Notice', 'complète', 'Titre']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Charger le modèle Stanza\n",
    "def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n",
    "    try:\n",
    "        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n",
    "    except:\n",
    "        stanza.download(lang=lang, logging_level='DEBUG')\n",
    "        nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n",
    "    return nlp\n",
    "\n",
    "# Lire le texte à partir d'un fichier\n",
    "def lire_fichier(chemin, is_json=False):\n",
    "    with open(chemin, encoding='utf-8') as f:\n",
    "        if is_json:\n",
    "            return json.load(f)\n",
    "        else:\n",
    "            return f.read().strip()\n",
    "\n",
    "# Obtenir un dictionnaire représentant une entité nommée (modifié selon la structure demandée)\n",
    "def get_ent_dict(ent) -> dict:\n",
    "    return {\"mot\": ent.text, \"type\": ent.type}\n",
    "\n",
    "# Traiter le texte et retourner une liste d'entités sans doublons\n",
    "def dico_resultats(text, lang: str = \"fr\") -> dict:\n",
    "    nlp = load_stanza_model(lang=lang)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    entites_unique = []\n",
    "    seen_entities = set()\n",
    "\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        entity_key = (ent.text, ent.type)\n",
    "        if entity_key not in seen_entities:\n",
    "            entites_unique.append(get_ent_dict(ent))  # Stocke uniquement {\"mot\":..., \"type\":...}\n",
    "            seen_entities.add(entity_key)\n",
    "\n",
    "    return entites_unique\n",
    "\n",
    "# Obtenir les étiquettes au format BIO\n",
    "def bio_stanza(text: str, lang: str = \"fr\") -> list[str]:\n",
    "    nlp = load_stanza_model(lang=lang)\n",
    "    doc = nlp(text)\n",
    "    return [\n",
    "        [token.text, token.ner] for sentence in doc.sentences for token in sentence.tokens\n",
    "    ]\n",
    "\n",
    "def remove_punctuation(token):\n",
    "    return not re.match(r'[\\W_]+', token)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Chemin du fichier d'entrée\n",
    "    file_path = \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\"\n",
    "    texte = lire_fichier(file_path)\n",
    "\n",
    "    # Extraire les entités nommées\n",
    "    entites = dico_resultats(texte, lang=\"fr\")\n",
    "    print(f\"Entités : {len(entites)}\")\n",
    "    print(entites[:20])  # Affiche les 20 premières entités\n",
    "\n",
    "    # Afficher les tokens obtenus par Stanza\n",
    "    nlp = load_stanza_model(lang=\"fr\")\n",
    "    doc = nlp(texte)\n",
    "    print(\"\\nTokens ：\")\n",
    "    tokens = [token.text for sentence in doc.sentences for token in sentence.tokens]\n",
    "    print(len(tokens))\n",
    "    for token in tokens[:20]:\n",
    "        print(token)\n",
    "\n",
    "    # Afficher les tokens non reconnus comme entité : candidats\n",
    "    entite_mots = set(ent[\"mot\"] for ent in entites)\n",
    "    candidats = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in entite_mots and remove_punctuation(token):\n",
    "            candidats.append(token)\n",
    "\n",
    "    print(\"\\nCandidats ：\")\n",
    "    print(len(candidats))\n",
    "    print(candidats[:20])  # Afficher les 20 premiers candidats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b0f8e-5f48-46c3-9177-686eefaaad1b",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "3f52980bfdaf463cb42e67f347fcc04c"
     ]
    },
    "id": "b3ea2ea1-1652-44bb-8f5b-606a7cf36c50",
    "outputId": "2c9c0e5b-f601-4b86-ec6e-67fee1df2bcb",
    "scrolled": true
   },
   "source": [
    "##### Parfois, Anaconda rencontre des problèmes et j'ai besoin de saisir manuellement le chemin.\n",
    "    file_paths = [\n",
    "        \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/DARBOUVILLE_Poesies-et-nouvelles.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/DESBORDES-VALMORE_Poesies-1820.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/HUGO_Contemplations-T2.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/LOISEAU_Fleurs-d-avril.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/NOAILLES_Derniers-vers.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/VERLAINE_Sagesse.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/SAUVAGE_Tandis-que-la-terre-tourne.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/VIVIEN_Etudes-et-preludes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc7dcace-e9ba-4129-a2a5-0de6cf9fb6d7",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "acc2d427d406443f87fee51ae66cf3e6"
     ]
    },
    "id": "dc7dcace-e9ba-4129-a2a5-0de6cf9fb6d7",
    "outputId": "b4d8b334-9865-45a8-d232-2a014483dfc1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:42:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File paths to process:\n",
      "/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\n",
      "/Users/zhengruixing/Desktop/DARBOUVILLE_Poesies-et-nouvelles.txt\n",
      "/Users/zhengruixing/Desktop/DESBORDES-VALMORE_Poesies-1820.txt\n",
      "/Users/zhengruixing/Desktop/HUGO_Contemplations-T2.txt\n",
      "/Users/zhengruixing/Desktop/LOISEAU_Fleurs-d-avril.txt\n",
      "/Users/zhengruixing/Desktop/NOAILLES_Derniers-vers.txt\n",
      "/Users/zhengruixing/Desktop/RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine.txt\n",
      "/Users/zhengruixing/Desktop/VERLAINE_Sagesse.txt\n",
      "/Users/zhengruixing/Desktop/SAUVAGE_Tandis-que-la-terre-tourne.txt\n",
      "/Users/zhengruixing/Desktop/VIVIEN_Etudes-et-preludes.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e5e3fdc07c433f989563f8b1c88710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:42:57 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n",
      "2025-04-27 15:42:57 WARNING: Language fr package default expects mwt, which has been added\n",
      "2025-04-27 15:42:58 INFO: Loading these models for language: fr (French):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | combined           |\n",
      "| mwt       | combined           |\n",
      "| ner       | wikinergold_charlm |\n",
      "==================================\n",
      "\n",
      "2025-04-27 15:42:58 INFO: Using device: cpu\n",
      "2025-04-27 15:42:58 INFO: Loading: tokenize\n",
      "2025-04-27 15:42:58 INFO: Loading: mwt\n",
      "2025-04-27 15:42:58 INFO: Loading: ner\n",
      "2025-04-27 15:43:00 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités : 8116\n",
      "[{'mot': 'Calligrammes', 'type': 'MISC'}, {'mot': 'Guillaume Apollinaire', 'type': 'PER'}, {'mot': 'La Fresnaye', 'type': 'PER'}, {'mot': 'Apollinaire', 'type': 'PER'}, {'mot': 'Guillaume', 'type': 'PER'}, {'mot': 'Lausanne', 'type': 'LOC'}, {'mot': 'Roger de', 'type': 'PER'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Description : Collection : Collection du Bouquet', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'GUILLAUME APOLLINAIRE', 'type': 'PER'}, {'mot': 'CALLIGRAMMES', 'type': 'PER'}, {'mot': 'M E R M O D', 'type': 'LOC'}, {'mot': 'RENÉ DALIZE', 'type': 'PER'}]\n",
      "\n",
      "Candidats : 232577\n",
      "['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages', '262', 'Notice', 'complète', 'Titre']\n",
      "\n",
      "Tokens : 296191\n",
      "['Rappel', 'de', 'votre', 'demande', ':', 'Format', 'de', 'téléchargement', ':', ':', 'Texte', 'Vues', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# Charger le modèle Stanza une fois pour复用\n",
    "def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n",
    "    nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n",
    "    return nlp\n",
    "\n",
    "# Lire le texte à partir d'un fichier\n",
    "def lire_fichier(chemin, is_json=False):\n",
    "    with open(chemin, encoding='utf-8') as f:\n",
    "        if is_json:\n",
    "            return json.load(f)\n",
    "        else:\n",
    "            return f.read().strip()\n",
    "\n",
    "# Obtenir un dictionnaire représentant une entité nommée (modifié selon la structure demandée)\n",
    "def get_ent_dict(ent) -> dict:\n",
    "    return {\"mot\": ent.text, \"type\": ent.type}\n",
    "\n",
    "# Traiter le texte et retourner une liste d'entités sans doublons\n",
    "def dico_resultats(text, nlp) -> dict:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    entites_unique = []\n",
    "    seen_entities = set()\n",
    "\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        entity_key = (ent.text, ent.type)\n",
    "        if entity_key not in seen_entities:\n",
    "            entites_unique.append(get_ent_dict(ent))  # Stocke uniquement {\"mot\":..., \"type\":...}\n",
    "            seen_entities.add(entity_key)\n",
    "\n",
    "    return entites_unique\n",
    "\n",
    "# 去掉标点符号的函数\n",
    "def remove_punctuation(token):\n",
    "    return not re.match(r'[\\W_]+', token)  # 判断是否是标点符号，若是标点符号则返回 False\n",
    "\n",
    "# 处理整个corpus的函数\n",
    "def process_corpus(file_paths, nlp):\n",
    "    all_entites = []\n",
    "    all_candidats = []\n",
    "    all_tokens = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        texte = lire_fichier(file_path)\n",
    "\n",
    "        # 获取命名实体\n",
    "        entites = dico_resultats(texte, nlp)\n",
    "        all_entites.extend(entites)  # 累积所有命名实体\n",
    "\n",
    "        # 获取 tokens\n",
    "        doc = nlp(texte)\n",
    "        tokens = [token.text for sentence in doc.sentences for token in sentence.tokens]\n",
    "        all_tokens.extend(tokens)  # 累积所有 tokens\n",
    "\n",
    "        # 获取未被识别为命名实体的词（candidats）\n",
    "        entite_mots = set(ent[\"mot\"] for ent in entites)\n",
    "        for token in tokens:\n",
    "            if token not in entite_mots and remove_punctuation(token):  # 过滤标点符号\n",
    "                all_candidats.append(token)  # 累积未识别的词\n",
    "\n",
    "    return all_entites, all_candidats, all_tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 使用glob获取目录下的所有txt文件\n",
    "    additional_file_paths = glob.glob(\"/Users/zhengruixing/Desktop/Corpus/*.txt\")\n",
    "\n",
    "    # 合并手动列出的路径和通过glob获取的路径\n",
    "    file_paths = manual_file_paths + additional_file_paths\n",
    "\n",
    "    # 输出文件路径确认\n",
    "    print(\"File paths to process:\")\n",
    "    for path in file_paths:\n",
    "        print(path)\n",
    "\n",
    "    # 先加载Stanza模型\n",
    "    nlp = load_stanza_model(lang=\"fr\")\n",
    "\n",
    "    # 处理整个语料库\n",
    "    all_entites, all_candidats, all_tokens = process_corpus(file_paths, nlp)\n",
    "\n",
    "    # 输出结果\n",
    "    print(f\"Entités : {len(all_entites)}\")\n",
    "    print(all_entites[:20])  # 打印前20个命名实体\n",
    "\n",
    "    print(f\"\\nCandidats : {len(all_candidats)}\")\n",
    "    print(all_candidats[:20])  # 打印前20个未识别的词\n",
    "\n",
    "    print(f\"\\nTokens : {len(all_tokens)}\")\n",
    "    print(all_tokens[:20])  # 打印前20个tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b08b8d-f4d5-43db-8e7e-cdcf68761575",
   "metadata": {
    "id": "28b08b8d-f4d5-43db-8e7e-cdcf68761575"
   },
   "outputs": [],
   "source": [
    " result = {\n",
    "        \"entites\": all_entites,\n",
    "        #\"candidats\": all_candidats,\n",
    "        #\"tokens\": all_tokens\n",
    "    }\n",
    "\n",
    "\n",
    "with open(\"/Users/zhengruixing/Desktop/0411/tous_entites_nommes_Stanza.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(result, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46763593-71d6-46b5-8ef8-5a6393c1eac8",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "655b7e6057754a16b2231964fb3a6abf"
     ]
    },
    "id": "46763593-71d6-46b5-8ef8-5a6393c1eac8",
    "outputId": "6025be62-f7e5-4dff-b924-7da78d2e13a3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 07:37:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655b7e6057754a16b2231964fb3a6abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 07:37:59 INFO: Downloaded file to /Users/zhengruixing/stanza_resources/resources.json\n",
      "2025-04-11 07:37:59 WARNING: Language fr package default expects mwt, which has been added\n",
      "2025-04-11 07:38:00 INFO: Loading these models for language: fr (French):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | combined           |\n",
      "| mwt       | combined           |\n",
      "| ner       | wikinergold_charlm |\n",
      "==================================\n",
      "\n",
      "2025-04-11 07:38:00 INFO: Using device: cpu\n",
      "2025-04-11 07:38:00 INFO: Loading: tokenize\n",
      "2025-04-11 07:38:00 INFO: Loading: mwt\n",
      "2025-04-11 07:38:00 INFO: Loading: ner\n",
      "2025-04-11 07:38:02 INFO: Done loading processors!\n",
      "Processing files:  10%|██▍                     | 1/10 [00:31<04:47, 31.99s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : APPOLINAIRE_Caligrammes\n",
      "  Tokens: 16683\n",
      "  Entités: 584\n",
      "  Candidats: 16004\n",
      "  Temps de traitement : 31.99 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Calligrammes', 'type': 'MISC'}, {'mot': 'Guillaume Apollinaire', 'type': 'PER'}, {'mot': 'La Fresnaye', 'type': 'PER'}, {'mot': 'Apollinaire', 'type': 'PER'}, {'mot': 'Guillaume', 'type': 'PER'}, {'mot': 'Lausanne', 'type': 'LOC'}, {'mot': 'Roger de', 'type': 'PER'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Description : Collection : Collection du Bouquet', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'GUILLAUME APOLLINAIRE', 'type': 'PER'}, {'mot': 'CALLIGRAMMES', 'type': 'PER'}, {'mot': 'M E R M O D', 'type': 'LOC'}, {'mot': 'RENÉ DALIZE', 'type': 'PER'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '262', 'sur', '262', 'Nombre', 'de', 'pages', '262', 'Notice', 'complète', 'Titre']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  20%|████▊                   | 2/10 [02:10<09:31, 71.38s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : DARBOUVILLE_Poesies-et-nouvelles\n",
      "  Tokens: 55462\n",
      "  Entités: 1064\n",
      "  Candidats: 42992\n",
      "  Temps de traitement : 98.94 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Titre : Poésies et nouvelles', 'type': 'MISC'}, {'mot': \"Madame d'Arbouville\", 'type': 'PER'}, {'mot': \"Sophie d'Arbouville\", 'type': 'PER'}, {'mot': 'P. de Barante', 'type': 'PER'}, {'mot': 'Arbouville', 'type': 'PER'}, {'mot': 'Sophie', 'type': 'PER'}, {'mot': 'Amyot', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Barante', 'type': 'PER'}, {'mot': 'Prosper Brugière', 'type': 'PER'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'NOUVELLES', 'type': 'ORG'}, {'mot': 'TYPOGRAPHIE DE CH. LAHURE Imprimeur du Sénat', 'type': 'ORG'}, {'mot': 'Cour de Cassation rue de Vaugirard', 'type': 'ORG'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '410', 'sur', '410', 'Nombre', 'de', 'pages', '410', 'Notice', 'complète', 'Titre']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  30%|███████▏                | 3/10 [03:03<07:19, 62.72s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : DESBORDES-VALMORE_Poesies-1820\n",
      "  Tokens: 30825\n",
      "  Entités: 905\n",
      "  Candidats: 23758\n",
      "  Temps de traitement : 52.42 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Poésies', 'type': 'MISC'}, {'mot': 'Mme Desbordes-Valmore', 'type': 'PER'}, {'mot': 'Desbordes-Valmore', 'type': 'PER'}, {'mot': 'Marceline', 'type': 'PER'}, {'mot': 'F. Louis', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Réserve des livres rares', 'type': 'LOC'}, {'mot': 'RES P-YE-785', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'Gars', 'type': 'LOC'}, {'mot': 'FRANÇOIS LOUIS', 'type': 'PER'}, {'mot': 'G1E S', 'type': 'LOC'}, {'mot': 'ELEGIES', 'type': 'ORG'}, {'mot': 'VVV', 'type': 'MISC'}, {'mot': 'T', 'type': 'MISC'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '213', 'sur', '213', 'Nombre', 'de', 'pages', '213', 'Notice', 'complète', 'Titre']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  40%|█████████▌              | 4/10 [05:01<08:28, 84.77s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : HUGO_Contemplations-T2\n",
      "  Tokens: 65385\n",
      "  Entités: 1531\n",
      "  Candidats: 47156\n",
      "  Temps de traitement : 118.55 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Titre : Les contemplations', 'type': 'MISC'}, {'mot': 'Victor Hugo', 'type': 'PER'}, {'mot': 'Hugo', 'type': 'PER'}, {'mot': 'Victor', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'LES', 'type': 'ORG'}, {'mot': 'CONTEMPLATIONS', 'type': 'PER'}, {'mot': 'Ai!', 'type': 'MISC'}, {'mot': 'VICTOR HUGO', 'type': 'PER'}, {'mot': 'TOME li', 'type': 'PER'}, {'mot': 'Jk i J', 'type': 'MISC'}, {'mot': 'D I R. B', 'type': 'MISC'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '246', 'sur', '246', 'Nombre', 'de', 'pages', '246', 'Notice', 'complète', 'Titre']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|████████████            | 5/10 [05:40<05:41, 68.27s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : LOISEAU_Fleurs-d-avril\n",
      "  Tokens: 20602\n",
      "  Entités: 538\n",
      "  Candidats: 16336\n",
      "  Temps de traitement : 39.03 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': \"Titre : Fleurs d'avril\", 'type': 'MISC'}, {'mot': 'Jeanne Loiseau', 'type': 'PER'}, {'mot': 'Loiseau', 'type': 'PER'}, {'mot': 'Jeanne', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': '8-YE-137', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'JEANNE LOISEAU', 'type': 'PER'}, {'mot': 'tVi', 'type': 'ORG'}, {'mot': 'Fleurs d’Avril', 'type': 'MISC'}, {'mot': 'POESIES', 'type': 'ORG'}, {'mot': 'SŸi', 'type': 'PER'}, {'mot': 'I H K l', 'type': 'LOC'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '136', 'sur', '136', 'Nombre', 'de', 'pages', '136', 'Notice', 'complète', 'Titre']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  60%|██████████████▍         | 6/10 [06:09<03:39, 54.75s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : NOAILLES_Derniers-vers\n",
      "  Tokens: 12820\n",
      "  Entités: 479\n",
      "  Candidats: 10431\n",
      "  Temps de traitement : 28.49 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Titre : Derniers vers', 'type': 'MISC'}, {'mot': 'Comtesse de Noailles', 'type': 'PER'}, {'mot': 'Noailles', 'type': 'PER'}, {'mot': 'Anna', 'type': 'PER'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Réserve des livres rares', 'type': 'LOC'}, {'mot': 'RES G-YE-122', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'COMTESSE DE NOAILLES', 'type': 'PER'}, {'mot': 'GRASSET', 'type': 'PER'}, {'mot': 'DERNIERS', 'type': 'ORG'}, {'mot': 'ANNE-JULES & HÉLÈNE', 'type': 'ORG'}, {'mot': 'Constantin PHOTIADÈS', 'type': 'PER'}, {'mot': 'AVERTISSEMENT', 'type': 'ORG'}, {'mot': 'comtesse de Noailles', 'type': 'PER'}, {'mot': 'Poème de P amour', 'type': 'MISC'}, {'mot': 'V Honneur', 'type': 'MISC'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'Format', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '166', 'sur', '166', 'Nombre', 'de', 'pages', '166', 'Notice', 'complète']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  70%|████████████████▊       | 7/10 [06:53<02:33, 51.16s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine\n",
      "  Tokens: 20685\n",
      "  Entités: 463\n",
      "  Candidats: 16622\n",
      "  Temps de traitement : 43.77 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Titre : les Illuminations', 'type': 'MISC'}, {'mot': 'Une Saison en Enfer', 'type': 'MISC'}, {'mot': 'Paul Verlaine', 'type': 'PER'}, {'mot': 'Titre : Une Saison en Enfer', 'type': 'MISC'}, {'mot': 'Rimbaud', 'type': 'PER'}, {'mot': 'Arthur', 'type': 'PER'}, {'mot': 'L. Vanier', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Verlaine', 'type': 'PER'}, {'mot': 'Paul', 'type': 'PER'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Réserve des livres rares', 'type': 'LOC'}, {'mot': 'RESP-Z-2180', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'UNE SAISON', 'type': 'MISC'}, {'mot': 'ENFER', 'type': 'ORG'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'Format', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '169', 'sur', '169', 'Nombre', 'de', 'pages', '169', 'Notice', 'complète']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  80%|███████████████████▏    | 8/10 [07:20<01:27, 43.54s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : VERLAINE_Sagesse\n",
      "  Tokens: 14122\n",
      "  Entités: 603\n",
      "  Candidats: 11213\n",
      "  Temps de traitement : 27.22 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Titre : Sagesse', 'type': 'MISC'}, {'mot': 'Paul Verlaine', 'type': 'PER'}, {'mot': 'Verlaine', 'type': 'PER'}, {'mot': 'Paul', 'type': 'PER'}, {'mot': 'L. Vanier', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Sagesse', 'type': 'PER'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'PAUL VERLAINE', 'type': 'PER'}, {'mot': 'SAGESSE', 'type': 'ORG'}, {'mot': 'REVUE El CORUIUKE', 'type': 'ORG'}, {'mot': 'PARIS', 'type': 'ORG'}, {'mot': 'EH', 'type': 'ORG'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '167', 'sur', '167', 'Nombre', 'de', 'pages', '167', 'Notice', 'complète', 'Titre']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  90%|█████████████████████▌  | 9/10 [08:02<00:42, 42.98s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : SAUVAGE_Tandis-que-la-terre-tourne\n",
      "  Tokens: 20726\n",
      "  Entités: 481\n",
      "  Candidats: 18190\n",
      "  Temps de traitement : 41.73 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Titre : Tandis que la terre', 'type': 'MISC'}, {'mot': 'Cécile Sauvage', 'type': 'PER'}, {'mot': 'Sauvage', 'type': 'PER'}, {'mot': 'Cécile', 'type': 'PER'}, {'mot': 'Mercure de France', 'type': 'MISC'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'CÉCILE SAUVAGE', 'type': 'PER'}, {'mot': 'POEMES', 'type': 'MISC'}, {'mot': 'PARIS MERCVRE DE FRANGE XXVI', 'type': 'ORG'}, {'mot': 'RVE DE CONDÉ', 'type': 'ORG'}, {'mot': 'XXVI', 'type': 'ORG'}, {'mot': 'Hollande', 'type': 'PER'}, {'mot': 'TIRAGE', 'type': 'ORG'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '204', 'sur', '204', 'Nombre', 'de', 'pages', '204', 'Notice', 'complète', 'Titre']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|███████████████████████| 10/10 [09:17<00:00, 55.75s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier : VIVIEN_Etudes-et-preludes\n",
      "  Tokens: 38881\n",
      "  Entités: 1468\n",
      "  Candidats: 29875\n",
      "  Temps de traitement : 75.34 secondes\n",
      "--------------------\n",
      "  20 premières entités : [{'mot': 'Titre : Poèmes de Renée Vivien : Études et préludes', 'type': 'MISC'}, {'mot': 'Cendres et poussières', 'type': 'MISC'}, {'mot': 'Évocations', 'type': 'MISC'}, {'mot': 'Sapho', 'type': 'PER'}, {'mot': 'La Vénus des aveugles', 'type': 'MISC'}, {'mot': 'Vivien', 'type': 'PER'}, {'mot': 'Renée', 'type': 'PER'}, {'mot': 'A. Lemerre', 'type': 'PER'}, {'mot': 'Paris', 'type': 'LOC'}, {'mot': 'Langue : Français', 'type': 'MISC'}, {'mot': 'Format', 'type': 'MISC'}, {'mot': 'XML DTBook', 'type': 'MISC'}, {'mot': 'Droits : Public domain', 'type': 'MISC'}, {'mot': 'Identifiant', 'type': 'MISC'}, {'mot': 'Bibliothèque nationale de France', 'type': 'LOC'}, {'mot': 'Littérature et art', 'type': 'ORG'}, {'mot': 'Conservation numérique', 'type': 'MISC'}, {'mot': 'OCR', 'type': 'MISC'}, {'mot': 'POÈMES', 'type': 'MISC'}, {'mot': 'D F', 'type': 'MISC'}]\n",
      "  20 premiers candidats : ['Rappel', 'de', 'votre', 'demande', 'de', 'téléchargement', 'Texte', 'Vues', '1', 'à', '270', 'sur', '270', 'Nombre', 'de', 'pages', '270', 'Notice', 'complète', 'Titre']\n",
      "========================================\n",
      "Traitement terminé pour tous les fichiers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm  # Importer la bibliothèque tqdm\n",
    "import time  # Pour calculer le temps de traitement de chaque fichier\n",
    "\n",
    "# Charger le modèle Stanza une fois pour réutilisation\n",
    "def load_stanza_model(lang: str = \"fr\") -> stanza.Pipeline:\n",
    "    nlp = stanza.Pipeline(lang=lang, processors='tokenize,ner')\n",
    "    return nlp\n",
    "\n",
    "# Lire le texte à partir d'un fichier\n",
    "def lire_fichier(chemin, is_json=False):\n",
    "    with open(chemin, encoding='utf-8') as f:\n",
    "        if is_json:\n",
    "            return json.load(f)\n",
    "        else:\n",
    "            return f.read().strip()\n",
    "\n",
    "# Obtenir un dictionnaire représentant une entité nommée (modifié selon la structure demandée)\n",
    "def get_ent_dict(ent) -> dict:\n",
    "    return {\"mot\": ent.text, \"type\": ent.type}\n",
    "\n",
    "# Traiter le texte et retourner une liste d'entités sans doublons\n",
    "def dico_resultats(text, nlp) -> dict:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    entites_unique = []\n",
    "    seen_entities = set()\n",
    "\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        entity_key = (ent.text, ent.type)\n",
    "        if entity_key not in seen_entities:\n",
    "            entites_unique.append(get_ent_dict(ent))  # Stocke uniquement {\"mot\":..., \"type\":...}\n",
    "            seen_entities.add(entity_key)\n",
    "\n",
    "    return entites_unique\n",
    "\n",
    "# Fonction pour supprimer la ponctuation\n",
    "def remove_punctuation(token):\n",
    "    return not re.match(r'[\\W_]+', token)  # Vérifie si c'est un signe de ponctuation, retourne False si c'est le cas\n",
    "\n",
    "# Traiter l'ensemble du corpus\n",
    "def process_corpus(file_paths, output_dir, nlp):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Créer le répertoire de sortie si il n'existe pas\n",
    "\n",
    "    # Utiliser tqdm pour afficher une barre de progression\n",
    "    for file_path in tqdm(file_paths, desc=\"Traitement des fichiers\", unit=\"fichier\"):\n",
    "        start_time = time.time()  # Enregistrer l'heure de début du traitement\n",
    "\n",
    "        # Obtenir le nom du fichier\n",
    "        file_name = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "        # Lire le contenu du fichier\n",
    "        texte = lire_fichier(file_path)\n",
    "\n",
    "        # Extraire les entités nommées\n",
    "        entites = dico_resultats(texte, nlp)\n",
    "\n",
    "        # Extraire les tokens\n",
    "        doc = nlp(texte)\n",
    "        tokens = [token.text for sentence in doc.sentences for token in sentence.tokens]\n",
    "\n",
    "        # Extraire les mots qui ne sont pas des entités nommées (candidats)\n",
    "        entite_mots = set(ent[\"mot\"] for ent in entites)\n",
    "        candidats = [token for token in tokens if token not in entite_mots and remove_punctuation(token)]\n",
    "\n",
    "        # Générer les données de sortie\n",
    "        output_data = {\n",
    "            \"entites_nom\": entites,\n",
    "            \"candidats\": candidats\n",
    "            \"tokens\": tokens\n",
    "        }\n",
    "\n",
    "        # Sauvegarder les résultats dans un fichier JSON\n",
    "        json_file_path = os.path.join(output_dir, f\"{file_name}_entites_candidats.json\")\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(output_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        # Calculer et afficher le temps de traitement pour chaque fichier\n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"Fichier : {file_name}\")\n",
    "        print(f\"  Tokens: {len(tokens)}\")\n",
    "        print(f\"  Entités: {len(entites)}\")\n",
    "        print(f\"  Candidats: {len(candidats)}\")\n",
    "        print(f\"  Temps de traitement : {processing_time:.2f} secondes\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        # Afficher les 20 premières entités et candidats\n",
    "        print(f\"  20 premières entités : {entites[:20]}\")\n",
    "        print(f\"  20 premiers candidats : {candidats[:20]}\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Sauvegarder les entités dans un fichier texte\n",
    "        with open(os.path.join(output_dir, f\"{file_name}_entites.txt\"), \"w\", encoding=\"utf-8\") as entites_file:\n",
    "            for entite in entites:\n",
    "                entites_file.write(f\"{entite['mot']} - {entite['type']}\\n\")\n",
    "\n",
    "        # Sauvegarder les candidats dans un fichier texte\n",
    "        with open(os.path.join(output_dir, f\"{file_name}_candidats.txt\"), \"w\", encoding=\"utf-8\") as candidats_file:\n",
    "            for mot in candidats:\n",
    "                candidats_file.write(f\"{mot}\\n\")\n",
    "\n",
    "    # Afficher un message lorsque tous les fichiers sont traités\n",
    "    print(\"Traitement terminé pour tous les fichiers.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Liste des chemins des fichiers\n",
    "    file_paths = [\n",
    "        \"/Users/zhengruixing/Desktop/APPOLINAIRE_Caligrammes.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/DARBOUVILLE_Poesies-et-nouvelles.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/DESBORDES-VALMORE_Poesies-1820.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/HUGO_Contemplations-T2.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/LOISEAU_Fleurs-d-avril.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/NOAILLES_Derniers-vers.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/RIMBAUD_Illuminations-et-Une-saison-en-enfer-et-Notice-Paul-Verlaine.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/VERLAINE_Sagesse.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/SAUVAGE_Tandis-que-la-terre-tourne.txt\",\n",
    "        \"/Users/zhengruixing/Desktop/VIVIEN_Etudes-et-preludes.txt\"\n",
    "    ]\n",
    "\n",
    "    # Répertoire de sortie\n",
    "    output_dir = \"/Users/zhengruixing/Desktop/0411/Stanza_output\"\n",
    "\n",
    "    # Charger le modèle Stanza\n",
    "    nlp = load_stanza_model(lang=\"fr\")\n",
    "\n",
    "    # Traiter l'ensemble du corpus et sauvegarder les résultats\n",
    "    process_corpus(file_paths, output_dir, nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de98014-fd83-4b86-9588-faf6f9296251",
   "metadata": {
    "id": "9de98014-fd83-4b86-9588-faf6f9296251"
   },
   "source": [
    "### Exclure les limitations de Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50841f25-afdd-4edc-bf0b-ec215fc43ca1",
   "metadata": {
    "id": "50841f25-afdd-4edc-bf0b-ec215fc43ca1"
   },
   "source": [
    "Stanza utilise effectivement des modèles basés sur des architectures similaires à BERT (comme celles du cadre transformers), et ces modèles peuvent être soumis à une limitation de la longueur d'entrée, ce qui signifie que les textes plus longs risquent de dépasser cette limite. En conséquence, Stanza pourrait être amené à diviser les textes en segments plus petits afin de respecter cette limite de longueur d'entrée.\n",
    "\n",
    "Cependant, Stanza ne fournit pas de documentation précise sur la longueur maximale de chaque entrée pour chaque modèle qu'il utilise. En règle générale, cela dépend de l'implémentation du modèle sous-jacent. Pour la plupart des modèles, cette gestion est automatisée, ce qui signifie que Stanza s'occupe de diviser le texte en segments appropriés de manière interne, sans que l'utilisateur ait à se soucier de cette question. Cela garantit que le texte ne dépasse pas la longueur maximale autorisée par le modèle sous-jacent.\n",
    "\n",
    "Stanza 在其处理过程中确实会利用类似于 BERT 的模型（如 transformers 库的实现），因此也可能面临 最大输入长度 的限制。这意味着，在处理较长的文本时，Stanza 也可能将文本切分成较小的片段进行处理，以适应最大输入长度的要求。\n",
    "\n",
    "但是，Stanza 并没有明确的文档说明对每个输入句子的长度限制是多少，通常这依赖于底层模型的具体实现。大多数情况下，Stanza 会在其底层模型的基础上自动处理这一点，所以开发者不需要显式地拆分句子。其文本处理过程会进行适当的拆分，以避免超出模型的输入长度限制。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
