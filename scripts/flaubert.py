# -*- coding: utf-8 -*-
"""Flaubert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xF_dgM3iVQ4EaUM00Y4JRjvZ0fmGxIdY
"""

import glob
from google.colab import drive
drive.mount('/content/drive')

base_path = "/content/drive/My Drive/Poemes"

for path in glob.glob(base_path):
  print(path)

!pip install transformers torch

#On a besoin du packet sacraemoses pour le tokenizer du flaubert https://pypi.org/project/sacremoses/

!pip install sacremoses

# Avec le Flaubert 'flaubert/flaubert_large_cased" de Hugging Face

from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import os
import json
from tqdm import tqdm
model_name = "flaubert/flaubert_large_cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

nlp_ner = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

import os
import json
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from google.colab import drive
drive.mount('/content/drive')

# Charger le modèle FlauBERT pour le tokenizer et NER
tokenizer = AutoTokenizer.from_pretrained("flaubert/flaubert_base_cased")
nlp_ner = pipeline("ner", model="Jean-Baptiste/camembert-ner")

base_path = "/content/drive/My Drive/Poemes/"
dic = {}

entites_par_auteur = {}
tokens_par_auteur = {}

for autor in tqdm(os.listdir(base_path), desc="Autors", unit="auteur"):
    autor_path = os.path.join(base_path, autor)

    if os.path.isdir(autor_path):
        entites_par_auteur[autor] = 0
        tokens_par_auteur[autor] = 0

        for poema in tqdm(os.listdir(autor_path), desc=f"Poèmes de {autor}", unit="poème", leave=False):
            if poema.endswith(".txt"):
                file_path = os.path.join(autor_path, poema)

                with open(file_path, "r", encoding="utf-8") as f:
                    conteudo = f.read()

                tokens = tokenizer.tokenize(conteudo)
                nb_tokens = len(tokens)

                entidades = nlp_ner(conteudo)
                nb_entites = len(entidades)

                tokens_par_auteur[autor] += nb_tokens
                entites_par_auteur[autor] += nb_entites

                dic[poema] = {
                    "auteur": autor,
                    "nombre de tokens": nb_tokens,
                    "nombre d'entités nommées": nb_entites,
                    "entités nommées": [{"texte": ent["word"], "type": ent["entity"]} for ent in entidades],
                }

json_file_path = "/content/drive/My Drive/premier_resultat_flaubert.json"
with open(json_file_path, "w", encoding="utf-8") as json_file:
    json.dump(dic, json_file, ensure_ascii=False, indent=4)

print("\nNombre total de tokens par auteur :")
for auteur, nb_tokens in tokens_par_auteur.items():
    print(f"- {auteur}: {nb_tokens} tokens")

print("\nNombre total d'entités nommées par auteur :")
for auteur, nb_entites in entites_par_auteur.items():
    print(f"- {auteur}: {nb_entites} entités nommées")

total_tokens = sum(tokens_par_auteur.values())
total_entites = sum(entites_par_auteur.values())

print(f"\nNombre total de tokens dans tous les poèmes : {total_tokens}")
print(f"Nombre total d'entités nommées dans tous les poèmes : {total_entites}")

print(f"\nAnalyse terminée. Résultats enregistrés dans {json_file_path}.")

print(f"Nombre total de tokens : {sum(doc['nombre de tokens'] for doc in dic.values())}")
