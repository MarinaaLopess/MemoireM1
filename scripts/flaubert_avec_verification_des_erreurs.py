# -*- coding: utf-8 -*-
"""Flaubert_avec_verification_des_erreurs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kD6fwj5oWUW6BHHseyUlQcu71rBY7BYQ
"""

import glob
from google.colab import drive
drive.mount('/content/drive')

base_path = "/content/drive/My Drive/Poemes"

for path in glob.glob(base_path):
  print(path)

#bibliothèque sacremose pour tokeniser
!pip install sacremoses

#version de Flaubert encore avec le modèle NER existant basé sur CamemBERT : Jean-Baptiste/camembert-ner de Hugging Face

import os
import json
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

tokenizer = AutoTokenizer.from_pretrained("flaubert/flaubert_base_cased")
nlp_ner = pipeline("ner", model="Jean-Baptiste/camembert-ner")

base_path = "/content/drive/My Drive/Poemes/"
dic = {}

entites_par_auteur = {}
tokens_par_auteur = {}

for autor in tqdm(os.listdir(base_path), desc="Autors", unit="auteur"):
    autor_path = os.path.join(base_path, autor)

    if os.path.isdir(autor_path):
        entites_par_auteur[autor] = 0
        tokens_par_auteur[autor] = 0

        for poema in tqdm(os.listdir(autor_path), desc=f"Poèmes de {autor}", unit="poème", leave=False):
            if poema.endswith(".txt"):
                file_path = os.path.join(autor_path, poema)

                with open(file_path, "r", encoding="utf-8") as f:
                    conteudo = f.read()

                tokens = tokenizer.tokenize(conteudo)
                nb_tokens = len(tokens)

                try:
                    entidades = nlp_ner(conteudo)
                    nb_entites = len(entidades)

                    tokens_par_auteur[autor] += nb_tokens
                    entites_par_auteur[autor] += nb_entites

                    dic[poema] = {
                        "auteur": autor,
                        "nombre de tokens": nb_tokens,
                        "nombre d'entités nommées": nb_entites,
                        "entités nommées": [{"texte": ent["word"], "type": ent["entity"]} for ent in entidades],
                    }
                except Exception as e:
                    print(f"❗ Erreur lors du traitement de {poema} ({autor}) : {e}")

json_file_path = "/content/drive/My Drive/premier_resultat_flaubert.json"
with open(json_file_path, "w", encoding="utf-8") as json_file:
    json.dump(dic, json_file, ensure_ascii=False, indent=4)


print("\nNombre total de tokens par auteur :")
for auteur, nb_tokens in tokens_par_auteur.items():
    print(f"- {auteur}: {nb_tokens} tokens")

print("\nNombre total d'entités nommées par auteur :")
for auteur, nb_entites in entites_par_auteur.items():
    print(f"- {auteur}: {nb_entites} entités nommées")

total_tokens = sum(tokens_par_auteur.values())
total_entites = sum(entites_par_auteur.values())

print(f"\nNombre total de tokens dans tous les poèmes : {total_tokens}")
print(f"Nombre total d'entités nommées dans tous les poèmes : {total_entites}")

print(f"\n✅ Analyse terminée. Résultats enregistrés dans {json_file_path}.")